===== CLEAN REPO EXPORT =====

===== FILE: app_backend\main.py =====
# Story: ST-00-backend-api-availability
# Feature: FT-00-backend-fundamentals
# Epic: E00
# Purpose: Backend bootstrapping and health endpoint.

from typing import Any, Dict, List
import json
import os

from fastapi import FastAPI, Depends, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from sqlalchemy.orm import Session

from .db import Base, engine, SessionLocal
from . import models, schemas

# Import your existing SCV domain service
from src.services.client_profile.service import ClientProfileService

# ------------------------------------
# Initialise database
# ------------------------------------
Base.metadata.create_all(bind=engine)

app = FastAPI(title="Single Client View (SCV) Backend")

# ------------------------------------
# CORS (kept for optional Vite dev server use)
# ------------------------------------
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",
        "http://127.0.0.1:5173",
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ------------------------------------
# Serve built frontend (Vite dist)
# ------------------------------------
FRONTEND_DIST = os.path.abspath(
    os.path.join(os.path.dirname(__file__), "..", "app_frontend", "dist")
)

if os.path.isdir(FRONTEND_DIST):
    # Serve static assets (JS/CSS, etc.)
    app.mount(
        "/assets",
        StaticFiles(directory=os.path.join(FRONTEND_DIST, "assets")),
        name="assets",
    )

    @app.get("/", include_in_schema=False)
    async def serve_react_index():
        """Serve the built React single-page app."""
        index_path = os.path.join(FRONTEND_DIST, "index.html")
        return FileResponse(index_path)


# ------------------------------------
# DB session dependency
# ------------------------------------
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()


# ------------------------------------
# Load raw records for a client
# ------------------------------------
def _load_raw_records_for_client(db: Session, client_id: str) -> List[Dict[str, Any]]:
    rows: List[models.SourceRecord] = (
        db.query(models.SourceRecord)
        .filter(models.SourceRecord.client_id == client_id)
        .all()
    )

    records: List[Dict[str, Any]] = []
    for row in rows:
        try:
            payload = json.loads(row.payload_json)
        except json.JSONDecodeError:
            continue

        if "_source" not in payload:
            payload["_source"] = row.system

        records.append(payload)

    return records


# ------------------------------------
# Health check
# ------------------------------------
@app.get("/health", tags=["system"])
def health_check():
    return {"status": "ok"}


# ------------------------------------
# Ingestion endpoint
# ------------------------------------
@app.post("/ingest", response_model=schemas.SourceRecordRead, tags=["ingestion"])
def ingest_source_record(
    record_in: schemas.SourceRecordCreate,
    db: Session = Depends(get_db),
):
    existing = (
        db.query(models.SourceRecord)
        .filter(
            models.SourceRecord.client_id == record_in.client_id,
            models.SourceRecord.system == record_in.system,
        )
        .one_or_none()
    )

    payload_json = json.dumps(record_in.payload)

    if existing:
        existing.payload_json = payload_json
        db.add(existing)
        db.commit()
        db.refresh(existing)
        return existing

    row = models.SourceRecord(
        client_id=record_in.client_id,
        system=record_in.system,
        payload_json=payload_json,
    )
    db.add(row)
    db.commit()
    db.refresh(row)
    return row


# ------------------------------------
# List raw source records
# ------------------------------------
@app.get(
    "/clients/{client_id}/sources",
    response_model=list[schemas.SourceRecordRead],
    tags=["clients"],
)
def list_client_sources(
    client_id: str,
    db: Session = Depends(get_db),
):
    rows = (
        db.query(models.SourceRecord)
        .filter(models.SourceRecord.client_id == client_id)
        .all()
    )
    out: List[schemas.SourceRecordRead] = []
    for row in rows:
        out.append(
            schemas.SourceRecordRead(
                id=row.id,
                client_id=row.client_id,
                system=row.system,
                payload=json.loads(row.payload_json),
            )
        )
    return out


# ------------------------------------
# SCV Profile endpoint (LDM aligned)
# ------------------------------------
@app.get("/clients/{client_id}/profile", tags=["clients"])
def get_client_profile(
    client_id: str,
    db: Session = Depends(get_db),
):
    raw_records = _load_raw_records_for_client(db, client_id)

    service = ClientProfileService()
    profile = service.assemble_base_profile(client_id, raw_records)

    if not profile:
        raise HTTPException(status_code=404, detail="Profile could not be assembled")

    return profile




===== FILE: app_frontend\src\App.css =====
#root {
  max-width: 1280px;
  margin: 0 auto;
  padding: 2rem;
  text-align: center;
}

.logo {
  height: 6em;
  padding: 1.5em;
  will-change: filter;
  transition: filter 300ms;
}
.logo:hover {
  filter: drop-shadow(0 0 2em #646cffaa);
}
.logo.react:hover {
  filter: drop-shadow(0 0 2em #61dafbaa);
}

@keyframes logo-spin {
  from {
    transform: rotate(0deg);
  }
  to {
    transform: rotate(360deg);
  }
}

@media (prefers-reduced-motion: no-preference) {
  a:nth-of-type(2) .logo {
    animation: logo-spin infinite 20s linear;
  }
}

.card {
  padding: 2em;
}

.read-the-docs {
  color: #888;
}



===== FILE: app_frontend\src\App.jsx =====
import React, { useState } from "react";

const BACKEND_BASE_URL = "http://127.0.0.1:8000";

function App() {
  const [clientId, setClientId] = useState("");
  const [loading, setLoading] = useState(false);
  const [profile, setProfile] = useState(null);
  const [sources, setSources] = useState([]);
  const [error, setError] = useState("");

  const handleSubmit = async (e) => {
    e.preventDefault();
    if (!clientId.trim()) {
      setError("Please enter a Client ID.");
      return;
    }
    setError("");
    setLoading(true);
    setProfile(null);
    setSources([]);

    try {
      const profileRes = await fetch(
        `${BACKEND_BASE_URL}/clients/${encodeURIComponent(clientId)}/profile`
      );
      if (!profileRes.ok) {
        const detail = await profileRes.json().catch(() => ({}));
        throw new Error(detail.detail || "Failed to fetch profile.");
      }
      const profileJson = await profileRes.json();

      const sourcesRes = await fetch(
        `${BACKEND_BASE_URL}/clients/${encodeURIComponent(clientId)}/sources`
      );
      let sourcesJson = [];
      if (sourcesRes.ok) {
        sourcesJson = await sourcesRes.json();
      }

      setProfile(profileJson);
      setSources(sourcesJson);
    } catch (err) {
      console.error(err);
      setError(err.message || "Something went wrong.");
    } finally {
      setLoading(false);
    }
  };

  return (
    <div className="min-h-screen bg-gray-100">
      {/* Top bar */}
      <header className="bg-white shadow-sm">
        <div className="max-w-6xl mx-auto px-4 py-4 flex items-center justify-between">
          <div className="flex items-baseline gap-2">
            <span className="text-2xl font-heading text-halo-primary">
              Single Client View
            </span>
            <span className="text-sm font-body text-gray-500">
              MissionHalo MVP
            </span>
          </div>
          <span className="text-xs font-body text-gray-400">
            Backend: {BACKEND_BASE_URL}
          </span>
        </div>
      </header>

      {/* Main layout */}
      <main className="max-w-6xl mx-auto px-4 py-6">
        {/* Search card */}
        <section className="bg-white rounded-halo shadow-sm border border-gray-200 p-6 mb-6">
          <h2 className="font-heading text-lg text-gray-800 mb-4">
            Find client profile
          </h2>

          <form
            onSubmit={handleSubmit}
            className="flex flex-col md:flex-row gap-4 items-stretch md:items-end"
          >
            <div className="flex-1">
              <label
                htmlFor="clientId"
                className="block text-sm font-body text-gray-600 mb-1"
              >
                Client ID
              </label>
              <input
                id="clientId"
                type="text"
                value={clientId}
                onChange={(e) => setClientId(e.target.value)}
                className="w-full px-3 py-2 rounded-md border border-gray-300 focus:outline-none focus:ring-2 focus:ring-halo-primary focus:border-halo-primary font-body text-sm"
                placeholder="e.g. 123"
              />
            </div>

            <button
              type="submit"
              disabled={loading}
              className="inline-flex items-center justify-center px-4 py-2 rounded-md font-body text-sm font-medium bg-halo-primary text-white hover:bg-emerald-700 disabled:opacity-60 disabled:cursor-not-allowed transition-colors"
            >
              {loading ? "Loadingâ€¦" : "Get profile"}
            </button>
          </form>

          {error && (
            <p className="mt-3 text-sm text-orange-700 bg-orange-50 border border-orange-200 rounded-md px-3 py-2 font-body">
              {error}
            </p>
          )}
        </section>

        {/* Content grid */}
        <section className="grid grid-cols-1 lg:grid-cols-3 gap-6">
          {/* Profile card */}
          <div className="lg:col-span-2">
            <div className="bg-white rounded-halo shadow-sm border border-gray-200 p-6 h-full">
              <h3 className="font-heading text-lg text-gray-800 mb-4 flex items-center justify-between">
                <span>Client profile</span>
                {profile && (
                  <span className="text-xs font-body text-gray-500">
                    Client ID: {profile.client_id}
                  </span>
                )}
              </h3>

              {!profile && !loading && (
                <p className="text-sm font-body text-gray-500">
                  Enter a Client ID and click{" "}
                  <span className="font-medium">Get profile</span> to see the
                  assembled Single Client View.
                </p>
              )}

              {profile && (
                <div className="space-y-4">
                  {/* Core identity */}
                  <div className="bg-gray-50 rounded-lg p-4 border border-gray-200">
                    <h4 className="font-heading text-sm text-gray-700 mb-2">
                      Core identity
                    </h4>
                    <dl className="grid grid-cols-2 gap-y-2 text-sm font-body">
                      <div>
                        <dt className="text-gray-500">Name</dt>
                        <dd className="text-gray-900">
                          {profile.name || "â€”"}
                        </dd>
                      </div>
                      <div>
                        <dt className="text-gray-500">Email</dt>
                        <dd className="text-gray-900">
                          {profile.email || "â€”"}
                        </dd>
                      </div>
                      <div>
                        <dt className="text-gray-500">Country</dt>
                        <dd className="text-gray-900">
                          {profile.country || "â€”"}
                        </dd>
                      </div>
                    </dl>
                  </div>

                  {/* Identifiers */}
                  <div className="bg-gray-50 rounded-lg p-4 border border-gray-200">
                    <h4 className="font-heading text-sm text-gray-700 mb-2">
                      Identifiers
                    </h4>
                    {profile.identifiers && profile.identifiers.length > 0 ? (
                      <ul className="space-y-1 text-sm font-body">
                        {profile.identifiers.map((id, idx) => (
                          <li
                            key={idx}
                            className="flex justify-between border-b border-gray-200 last:border-b-0 pb-1"
                          >
                            <span className="text-gray-600">
                              {id.system || id["system"]}
                            </span>
                            <span className="text-gray-900 font-mono">
                              {id.value || id["value"]}
                            </span>
                          </li>
                        ))}
                      </ul>
                    ) : (
                      <p className="text-sm text-gray-500 font-body">
                        No identifiers found.
                      </p>
                    )}
                  </div>

                  {/* Addresses */}
                  <div className="bg-gray-50 rounded-lg p-4 border border-gray-200">
                    <h4 className="font-heading text-sm text-gray-700 mb-2">
                      Addresses
                    </h4>
                    {profile.addresses && profile.addresses.length > 0 ? (
                      <ul className="space-y-2 text-sm font-body">
                        {profile.addresses.map((addr, idx) => (
                          <li
                            key={idx}
                            className="border-b border-gray-200 pb-2 last:border-b-0"
                          >
                            <div className="text-gray-900">
                              {[
                                addr.line1 || addr["line1"],
                                addr.line2 || addr["line2"],
                                addr.city || addr["city"],
                                addr.postcode || addr["postcode"],
                                addr.country || addr["country"],
                              ]
                                .filter(Boolean)
                                .join(", ")}
                            </div>
                            <div className="text-xs text-gray-500 mt-1">
                              Source: {addr.source || addr["source"] || "â€”"}
                            </div>
                          </li>
                        ))}
                      </ul>
                    ) : (
                      <p className="text-sm text-gray-500 font-body">
                        No addresses available.
                      </p>
                    )}
                  </div>
                </div>
              )}
            </div>
          </div>

          {/* Raw sources / lineage card */}
          <div>
            <div className="bg-white rounded-halo shadow-sm border border-gray-200 p-6 h-full">
              <h3 className="font-heading text-lg text-gray-800 mb-4">
                Raw sources
              </h3>

              {!profile && !loading && (
                <p className="text-sm font-body text-gray-500">
                  Once a profile is loaded, you&apos;ll see the upstream source
                  records here.
                </p>
              )}

              {sources && sources.length > 0 && (
                <div className="space-y-3 text-xs font-mono bg-gray-50 rounded-lg p-3 border border-gray-200 max-h-[400px] overflow-auto">
                  {sources.map((src) => (
                    <div
                      key={src.id}
                      className="bg-white border border-gray-200 rounded-md p-2"
                    >
                      <div className="flex justify-between items-center mb-1">
                        <span className="font-body text-xs text-gray-600">
                          {src.system}
                        </span>
                        <span className="font-body text-[10px] text-gray-400">
                          client_id: {src.client_id}
                        </span>
                      </div>
                      <pre className="whitespace-pre-wrap text-[11px] text-gray-800">
                        {JSON.stringify(src.payload, null, 2)}
                      </pre>
                    </div>
                  ))}
                </div>
              )}

              {profile && profile.lineage && (
                <>
                  <h4 className="font-heading text-sm text-gray-700 mt-4 mb-2">
                    Lineage
                  </h4>
                  <div className="bg-gray-50 rounded-lg p-3 border border-gray-200 text-xs font-body max-h-[200px] overflow-auto">
                    <pre className="whitespace-pre-wrap text-[11px] text-gray-800">
                      {JSON.stringify(profile.lineage, null, 2)}
                    </pre>
                  </div>
                </>
              )}
            </div>
          </div>
        </section>
      </main>
    </div>
  );
}

export default App;





===== FILE: app_frontend\src\index.css =====
/* app_frontend/src/index.css */

/* This pulls in Tailwind v4 (base, components, utilities) */
@import "tailwindcss";

/* Simple global defaults â€“ Tailwind utilities handle the rest */
:root {
  font-family: system-ui, -apple-system, BlinkMacSystemFont, "Segoe UI",
    sans-serif;
}

*,
*::before,
*::after {
  box-sizing: border-box;
}

body {
  margin: 0;
  min-height: 100vh;
}




===== FILE: app_frontend\src\main.jsx =====
import { StrictMode } from 'react'
import { createRoot } from 'react-dom/client'
import './index.css'
import App from './App.jsx'

createRoot(document.getElementById('root')).render(
  <StrictMode>
    <App />
  </StrictMode>,
)



===== FILE: app_frontend\index.html =====
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/vite.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>SCV Frontend</title>

    <!-- MissionHalo fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;500;700&family=Raleway:wght@500;600;700&display=swap"
      rel="stylesheet"
    />
  </head>
  <body class="bg-gray-100">
    <div id="root"></div>
    <script type="module" src="/src/main.jsx"></script>
  </body>
</html>




===== FILE: app_frontend\package.json =====
{
  "name": "app_frontend",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "lint": "eslint .",
    "preview": "vite preview"
  },
  "dependencies": {
    "react": "^19.2.0",
    "react-dom": "^19.2.0"
  },
  "devDependencies": {
    "@eslint/js": "^9.39.1",
    "@tailwindcss/postcss": "^4.1.17",
    "@types/react": "^19.2.5",
    "@types/react-dom": "^19.2.3",
    "@vitejs/plugin-react": "^5.1.1",
    "autoprefixer": "^10.4.22",
    "eslint": "^9.39.1",
    "eslint-plugin-react-hooks": "^7.0.1",
    "eslint-plugin-react-refresh": "^0.4.24",
    "globals": "^16.5.0",
    "postcss": "^8.5.6",
    "tailwindcss": "^4.1.17",
    "vite": "^7.2.4"
  }
}



===== FILE: app_frontend\vite.config.js =====
import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'

// https://vite.dev/config/
export default defineConfig({
  plugins: [react()],
})



===== FILE: .github\workflows\ci.yml =====
name: Story CI

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  story-checks:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          python -m pip install pytest bandit ruff

      - name: Run Story tests and update testing_status
        run: |
          python tools/run_story_tests.py

      - name: Run Story guardrails and update guardrail_adherence
        run: |
          python tools/run_story_guardrails.py

      - name: Run Story security checks and update security_policy_adherence
        run: |
          python tools/run_story_security.py

      - name: Run Story lint/code-quality checks and update code_quality_adherence
        run: |
          python tools/run_story_lint.py

      - name: Ensure Story statuses are up to date (no drift)
        run: |
          # If running the Story scripts changed any Story files, CI fails.
          # This forces updated status fields to be committed before pushing.
          if ! git diff --quiet docs/mission_destination/stories; then
            echo "Story status fields are not in sync with checks."
            echo "Run locally:"
            echo "  python tools/run_story_tests.py"
            echo "  python tools/run_story_guardrails.py"
            echo "  python tools/run_story_security.py"
            echo "  python tools/run_story_lint.py"
            echo "Then commit the updated Story files before pushing."
            echo
            git diff -- docs/mission_destination/stories
            exit 1
          fi

      - name: Capture CI evidence (placeholder)
        run: |
          mkdir -p missionlog/test_evidence
          echo "CI run at $(date -u)" > missionlog/test_evidence/latest.txt




===== FILE: .github\workflows\story-tests.yml =====
name: Story Tests

on:
  workflow_dispatch:
  push:
    branches: [ main ]
  pull_request:

jobs:
  run-tests:
    runs-on: ubuntu-latest
    env:
      PYTHONPATH: ${{ github.workspace }}:${{ github.workspace }}/src

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Run tests
        id: tests
        run: pytest



===== FILE: docs\mission_destination\initial_logical_data_model.md =====
# Initial Logical Data Model (LDM)
**MissionDestination**
**Single Client View (SCV)**

## 1. Purpose
This document defines the **Initial Logical Data Model** for the Single Client View (SCV) application.
It establishes the **canonical semantic representation of a client**, which all ingestion,
normalisation, matching, merging, lineage, quality, and API features will converge toward.

The Initial Logical Data Model is:
- technology-agnostic
- storage-agnostic
- canonical across all source systems
- stable across all Stories
- the semantic target for the behaviours defined in Epics, Features, and Stories

It is **not** a physical database schema.  
It is the logical definition of what a â€œclientâ€ means inside the SCV domain.

## 2. Canonical Entities

### 2.1 ClientProfile
Represents the unified, deduplicated profile of a client.

| Field | Type | Description |
|-------|------|-------------|
| client_id | str | Internal SCV identifier |
| name | str | Canonical client name |
| email | Optional[str] | Canonical email |
| country | Optional[str] | Country associated with the client |
| identifiers | List[ClientIdentifier] | Identifiers from upstream systems |
| addresses | List[ClientAddress] | Normalised address objects |
| lineage | Dict[str, Any] | Provenance metadata |
| quality | Dict[str, float] | Freshness, completeness, confidence |
| metadata | Dict[str, str] | Timestamps, merge strategies, flags |
| raw_sources | Dict[str, Dict[str, Any]] | Raw source payloads |

### 2.2 ClientIdentifier
| Field | Type | Description |
|-------|------|-------------|
| system | str | Upstream system name |
| value | str | Identifier in that system |

### 2.3 ClientAddress
| Field | Type | Description |
|-------|------|-------------|
| line1 | Optional[str] | Address line 1 |
| line2 | Optional[str] | Address line 2 |
| city | Optional[str] | City |
| postcode | Optional[str] | Postal code |
| country | Optional[str] | ISO country |
| source | Optional[str] | System contributing the address |

## 3. Python Representation

```python
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any

@dataclass
class ClientIdentifier:
    system: str
    value: str

@dataclass
class ClientAddress:
    line1: Optional[str] = None
    line2: Optional[str] = None
    city: Optional[str] = None
    postcode: Optional[str] = None
    country: Optional[str] = None
    source: Optional[str] = None

@dataclass
class ClientProfile:
    client_id: str
    name: str

    email: Optional[str] = None
    country: Optional[str] = None

    identifiers: List[ClientIdentifier] = field(default_factory=list)
    addresses: List[ClientAddress] = field(default_factory=list)

    lineage: Dict[str, Any] = field(default_factory=dict)
    quality: Dict[str, float] = field(default_factory=dict)
    metadata: Dict[str, str] = field(default_factory=dict)

    raw_sources: Dict[str, Dict[str, Any]] = field(default_factory=dict)
```

## 4. Next Steps
- Add this model to `src/domain/models/client_profile.py`
- Keep structure stable through MVP
- Extend only if new Stories introduce semantic requirements



===== FILE: docs\mission_destination\initial_story_sequence.md =====
\# Initial Story Sequence (SCV)

\*\*MissionDestination\*\*  

\*\*Single Client View (SCV)\*\*



---



\## 1. Purpose



This document defines the \*\*initial execution order for all Stories\*\* in the Single Client View (SCV) MissionDestination.



The ordering is derived from:



\- the SCV Epics and Features,

\- the Initial Logical Data Model (LDM),

\- dependency analysis across services and APIs,

\- and the need to build up capability incrementally in a stable way.



It is a \*\*MissionDestination artefact\*\*: it describes \*how the business semantics are best realised over time\*, not runtime performance (MissionDynamics).



This sequence underpins:



\- developer and agent workflows,

\- CI/test evolution,

\- and the progressive construction of the canonical SCV capability.



---



\## 2. Relationship to Other Artefacts



This Story Sequence should be read together with:



\- \*\*MissionFramework\*\* (technology guardrails, testing strategy, CI blueprint, policy-as-code)

\- \*\*Meta-prompts\*\* (code, test, CI)

\- \*\*Instruction prompts\*\* (how Stories are executed)

\- \*\*MissionDestination\*\*:

&nbsp; - Epics, Features, Stories

&nbsp; - Initial Logical Data Model (LDM)

\- \*\*Mission Build Plan\*\* (the overall step-by-step setup of the project)



The sequence assumes that:



1\. The repo structure exists.  

2\. MissionFramework and meta-prompts are in place.  

3\. The Initial Logical Data Model for SCV (`ClientProfile`, `ClientIdentifier`, `ClientAddress`) is defined.  



Only then is it valid to implement Stories in this order.



---



\## 3. Golden Story Sequence (MVP Build Order)



The following is the \*\*recommended implementation sequence\*\* for the SCV Stories.



Each entry references the Story ID; the Story definition remains the single source of truth for behaviour and acceptance criteria.



1\. \*\*ST-03 â€“ Map identity fields\*\*  

2\. \*\*ST-04 â€“ Map identifiers\*\*  

3\. \*\*ST-20 â€“ Assemble base profile\*\*  



4\. \*\*ST-09 â€“ Match by tax ID\*\*  

5\. \*\*ST-10 â€“ Match by registration number\*\*  



6\. \*\*ST-16 â€“ Build search index\*\*  

7\. \*\*ST-17 â€“ Normalise search fields\*\*  

8\. \*\*ST-32 â€“ Implement basic search API\*\*  



9\. \*\*ST-18 â€“ Implement fuzzy search queries\*\*  

10\. \*\*ST-19 â€“ Implement search ranking\*\*  



11\. \*\*ST-13 â€“ Merge identity attributes\*\*  

12\. \*\*ST-14 â€“ Merge addresses\*\*  

13\. \*\*ST-15 â€“ Record merge lineage\*\*  



14\. \*\*ST-21 â€“ Assemble profile metadata\*\*  

15\. \*\*ST-22 â€“ Expose lineage in profile\*\*  

16\. \*\*ST-24 â€“ Flag merge conflicts\*\*  

17\. \*\*ST-25 â€“ Present merge logic / rationale\*\*  



18\. \*\*ST-28 â€“ Compute data freshness\*\*  

19\. \*\*ST-29 â€“ Compute data completeness\*\*  

20\. \*\*ST-26 â€“ Store lineage history\*\*  

21\. \*\*ST-27 â€“ Timestamp lineage events\*\*  



22\. \*\*ST-07 â€“ Detect upstream deltas\*\*  

23\. \*\*ST-08 â€“ Apply upstream deltas to SCV\*\*  



24\. \*\*ST-05 â€“ Bulk load from CRM\*\*  

25\. \*\*ST-06 â€“ Bulk load from KYC\*\*  



26\. \*\*ST-33 â€“ Refine search API ranking\*\*  

27\. \*\*ST-34 â€“ Implement basic client profile API\*\*  

28\. \*\*ST-35 â€“ Expose lineage via client profile API\*\*  



29\. \*\*ST-11 â€“ Fuzzy name matching\*\*  

30\. \*\*ST-12 â€“ Attribute-level confidence scoring\*\*  

31\. \*\*ST-23 â€“ Lineage drill-down / field-level provenance\*\*  



32\. \*\*ST-30 â€“ Audit ingestion operations\*\*  

33\. \*\*ST-31 â€“ Audit merge operations\*\*  



34\. \*\*ST-01 â€“ Register CRM source system\*\*  

35\. \*\*ST-02 â€“ Register KYC source system\*\*



---



\## 4. Usage and Change Control



\- This sequence is the \*\*default build order\*\* for human developers and agents.  

\- No Story should be implemented \*\*significantly out of sequence\*\* without re-running dependency analysis.  

\- If the Story set changes (new Stories, retired Stories, structural changes to Epics/Features), this document \*\*must be updated\*\* to reflect the new ordering.  



For each Story:



1\. Read the Story definition under `docs/mission\_destination/stories/`.  

2\. Implement the corresponding code slice(s) in `src/` as per the code meta-prompt.  

3\. Implement the corresponding tests in `tests/` as per the test meta-prompt.  

4\. Ensure CI passes and MissionFramework guardrails are satisfied.  

5\. Only then move to the next Story in this sequence.



---






===== FILE: docs\mission_destination\epics\E00_application_bootstrapping.md =====
---
epic_id: E00
slug: e00_application_bootstrapping
name: "Application Bootstrapping"
description: |
  Establish the initial backend service and runtime foundations required for the
  Single Client View application to start, respond to basic requests, and be
  governed by MissionFramework guardrails.

features:
  - FT-00

overall_status: active

testing_status: aggregated
halo_adherence: aggregated
guardrail_adherence: aggregated
code_quality_adherence: aggregated
security_policy_adherence: aggregated

feature_statuses: {}

last_updated: <auto>
---

## Context

This epic delivers the minimal backend scaffolding needed for developers to
build, run, and verify the Single Client View application.

## Objectives

- Backend starts reliably in a developer environment.
- At least one health/status endpoint is available.
- Code adheres to MissionFramework guardrails.
- Story-level test status is automatically updated.

## Acceptance

- All features under this epic have status `active` or `done`.
- All related stories have `testing_status: pass`.







===== FILE: docs\mission_destination\epics\E00_ui_bootstrapping.md =====
---

id: E00-UI

slug: e00\_ui\_bootstrapping

type: epic

name: User Interface Bootstrapping

status: active

description: >

&nbsp; Establish the initial frontend shell for the Single Client Value application,

&nbsp; ensuring that a basic UI loads, renders and is testable.

owner: TBD

related\_features:

&nbsp; - FT-00-frontend-fundamentals

---



\## Context



This epic delivers the minimal frontend scaffolding needed for the Single

Client Value UI to build, load in a browser, and render predictable output.



\## Objectives



\- Frontend builds without errors.

\- App renders a visible indicator confirming it is running.

\- Tests validate this behaviour.



\## Acceptance



\- All related features are `active` or `done`.

\- All related stories have `test\_status: passed`.






===== FILE: docs\mission_destination\epics\E01_client_ingestion_and_normalisation.md =====
---
epic_id: EP-01
name: "Client Ingestion & Normalisation"
description: |
  This epic delivers the capability to ingest client data from multiple internal
  systems (CRM, KYC, onboarding, trading) and normalise it into a single
  canonical client model. It ensures consistent formats, data types, and field
  structures across all sources. This is the pipeline that feeds the SCV engine
  with reliable, usable data.

features:
  - FT-01
  - FT-02
  - FT-03
  - FT-04

overall_status: Planned

testing_status: aggregated
halo_adherence: aggregated
guardrail_adherence: aggregated
code_quality_adherence: aggregated
security_policy_adherence: aggregated

feature_statuses: {}

last_updated: <auto>
---



===== FILE: docs\mission_destination\epics\E02_client_matching_and_golden_record.md =====
---
epic_id: EP-02
name: "Client Matching & Golden Record"
description: |
  This epic establishes the rules and logic for matching client records across
  systems, resolving duplicates, and building the authoritative golden record.
  Includes exact and fuzzy matching, conflict resolution, and merge lineage.
  The output becomes the Single Client View master profile.

features:
  - FT-05
  - FT-06
  - FT-07

overall_status: Planned

testing_status: aggregated
halo_adherence: aggregated
guardrail_adherence: aggregated
code_quality_adherence: aggregated
security_policy_adherence: aggregated

feature_statuses: {}

last_updated: <auto>
---



===== FILE: docs\mission_destination\epics\E03_client_search.md =====
---
epic_id: EP-03
name: "Client Search"
description: |
  This epic provides search capabilities for finding clients based on names,
  identifiers, partial information, or attributes. It includes indexing,
  relevance ranking, fuzzy matching, and result scoring. Search is the primary
  user entry point into the Single Client View.

features:
  - FT-08
  - FT-09

overall_status: Planned

testing_status: aggregated
halo_adherence: aggregated
guardrail_adherence: aggregated
code_quality_adherence: aggregated
security_policy_adherence: aggregated

feature_statuses: {}

last_updated: <auto>
---



===== FILE: docs\mission_destination\epics\E04_client_profile_assembly.md =====
---
epic_id: EP-04
name: "Client Profile Assembly"
description: |
  This epic assembles the unified client profile from the golden record,
  including core identity, operational attributes, source lineage, and data
  quality indicators. It defines how attribute-level provenance is exposed and
  how conflicting information is presented.

features:
  - FT-10
  - FT-11
  - FT-12

overall_status: Planned

testing_status: aggregated
halo_adherence: aggregated
guardrail_adherence: aggregated
code_quality_adherence: aggregated
security_policy_adherence: aggregated

feature_statuses: {}

last_updated: <auto>
---



===== FILE: docs\mission_destination\epics\E05_data_quality_and_lineage.md =====
---
epic_id: EP-05
name: "Data Quality & Lineage"
description: |
  This epic delivers attribute-level lineage tracking, data freshness indicators,
  quality scoring, conflict flags, and auditability. It ensures every attribute
  in the SCV profile can be traced back to its original source system with full
  transparency.

features:
  - FT-13
  - FT-14
  - FT-15

overall_status: Planned

testing_status: aggregated
halo_adherence: aggregated
guardrail_adherence: aggregated
code_quality_adherence: aggregated
security_policy_adherence: aggregated

feature_statuses: {}

last_updated: <auto>
---



===== FILE: docs\mission_destination\epics\E06_api_and_integration.md =====
---
epic_id: EP-06
name: "Integration & API Exposure"
description: |
  This epic exposes the SCV capabilities to other systems via APIs. Includes
  search endpoints, client profile retrieval, and interface contracts for
  internal consumers. Ensures consistent, secure, and performant access to the
  Single Client View.

features:
  - FT-16
  - FT-17

overall_status: Planned

testing_status: aggregated
halo_adherence: aggregated
guardrail_adherence: aggregated
code_quality_adherence: aggregated
security_policy_adherence: aggregated

feature_statuses: {}

last_updated: <auto>
---



===== FILE: docs\mission_destination\features\FT-00-backend_fundamentals.md =====
---
feature_id: FT-00
epic: E00
name: "Backend Fundamentals"
description: |
  Establish the initial backend service and runtime foundations required for the
  Single Client View application to start, respond to basic requests, and be
  governed by MissionFramework guardrails. This feature delivers a minimal,
  reliable API surface that other stories can build on.

stories:
  - ST-00

overall_status: Planned

testing_status: aggregated
halo_adherence: aggregated
guardrail_adherence: aggregated
code_quality_adherence: aggregated
security_policy_adherence: aggregated

story_statuses: {}

last_updated: <auto>
---



===== FILE: docs\mission_destination\features\FT-00-frontend_fundamentals.md =====
---

id: FT-00-frontend-fundamentals

slug: ft-00-frontend-fundamentals

type: feature

name: Frontend Fundamentals

epic\_id: E00-UI

status: active

description: >

&nbsp; Provide a minimal frontend shell that builds, loads, renders, and verifies the

&nbsp; baseline UI is operational.

owner: TBD

related\_stories:

&nbsp; - ST-00-frontend-ui-shell

---



\## Scope



\- Frontend build and dev server.

\- Root React component and layout shell.

\- Visible indicator that the UI is running.



\## Out of Scope



\- Domain UI for Single Client Value.

\- Search, profile, lineage, or matching components.






===== FILE: docs\mission_destination\features\FT-01_source_system_config.md =====
---
feature_id: FT-01
epic: EP-01
name: "Source System Configuration"
description: |
  Configure each upstream client data source, including credentials, endpoints,
  schema discovery, and scheduling parameters. Enables SCV to pull structured
  data from CRM, KYC, onboarding, and trading systems.

stories:
  - ST-01
  - ST-02

overall_status: Planned

testing_status: aggregated
halo_adherence: aggregated
guardrail_adherence: aggregated
code_quality_adherence: aggregated
security_policy_adherence: aggregated

story_statuses: {}

last_updated: <auto>
---



===== FILE: docs\mission_destination\features\FT-02_schema_mapping.md =====
---
feature_id: FT-02
epic: EP-01
name: "Schema Mapping to Canonical Model"
description: |
  Map source-system client schemas into SCVâ€™s canonical schema, including
  identity fields, contact details, identifiers, and operational attributes.

stories:
  - ST-03
  - ST-04

overall_status: Planned

testing_status: aggregated
halo_adherence: aggregated
guardrail_adherence: aggregated
code_quality_adherence: aggregated
security_policy_adherence: aggregated

story_statuses: {}

last_updated: <auto>
---



===== FILE: docs\mission_destination\features\FT-03_initial_bulk_ingestion.md =====
---
feature_id: FT-03
epic: EP-01
name: "Initial Bulk Ingestion"
description: |
  Perform a one-time historical load of all client records from each upstream
  system. Includes batching, error handling, and ingestion evidence.

stories:
  - ST-05
  - ST-06

overall_status: Planned

testing_status: aggregated
halo_adherence: aggregated
guardrail_adherence: aggregated
code_quality_adherence: aggregated
security_policy_adherence: aggregated

story_statuses: {}

last_updated: <auto>
---



===== FILE: docs\mission_destination\features\FT-04_incremental_ingestion.md =====
---
feature_id: FT-04
epic: EP-01
name: "Incremental Ingestion & Change Detection"
description: |
  Implement scheduled or event-driven incremental updates, detecting changes in
  upstream systems and applying deltas to SCVâ€™s internal store.

stories:
  - ST-07
  - ST-08

overall_status: Planned

testing_status: aggregated
halo_adherence: aggregated
guardrail_adherence: aggregated
code_quality_adherence: aggregated
security_policy_adherence: aggregated

story_statuses: {}

last_updated: <auto>
---



===== FILE: docs\mission_destination\features\FT-05_exact_matching.md =====
---
feature_id: FT-05
epic: EP-02
name: "Exact Match Rules"
description: |
  Implement deterministic matching rules based on unique identifiers such as
  client ID, company registration number, tax ID, and email address.

stories:
  - ST-09
  - ST-10

overall_status: Planned

testing_status: aggregated
halo_adherence: aggregated
guardrail_adherence: aggregated
code_quality_adherence: aggregated
security_policy_adherence: aggregated

story_statuses: {}

last_updated: <auto>
---



===== FILE: docs\mission_destination\features\FT-06_fuzzy_matching.md =====
---
feature_id: FT-06
epic: EP-02
name: "Fuzzy & Probabilistic Matching"
description: |
  Implement relevance-based matching using name similarity, token distance, and
  attribute-level confidence scoring.

stories:
  - ST-11
  - ST-12

overall_status: Planned

testing_status: aggregated
halo_adherence: aggregated
guardrail_adherence: aggregated
code_quality_adherence: aggregated
security_policy_adherence: aggregated

story_statuses: {}

last_updated: <auto>
---



===== FILE: docs\mission_destination\features\FT-07_golden_record_merge.md =====
---
feature_id: FT-07
epic: EP-02
name: "Golden Record Construction"
description: |
  Apply merge rules across conflicting source attributes, selecting canonical
  values and recording lineage for each field.

stories:
  - ST-13
  - ST-14
  - ST-15

overall_status: Planned

testing_status: aggregated
halo_adherence: aggregated
guardrail_adherence: aggregated
code_quality_adherence: aggregated
security_policy_adherence: aggregated

story_statuses: {}

last_updated: <auto>
---



===== FILE: docs\mission_destination\features\FT-08_search_indexing.md =====
---
feature_id: FT-08
epic: EP-03
name: "Search Index & Normalisation"
description: |
  Build a performant search index for names, identifiers, and attributes.

stories:
  - ST-16
  - ST-17

overall_status: Planned

testing_status: aggregated
halo_adherence: aggregated
guardrail_adherence: aggregated
code_quality_adherence: aggregated
security_policy_adherence: aggregated

story_statuses: {}

last_updated: <auto>
---



===== FILE: docs\mission_destination\features\FT-09_fuzzy_search.md =====
---
feature_id: FT-09
epic: EP-03
name: "Fuzzy Search & Ranking"
description: |
  Provide fuzzy search, relevance ranking, and partial-match scoring.

stories:
  - ST-18
  - ST-19

overall_status: Planned

testing_status: aggregated
halo_adherence: aggregated
guardrail_adherence: aggregated
code_quality_adherence: aggregated
security_policy_adherence: aggregated

story_statuses: {}

last_updated: <auto>
---



===== FILE: docs\mission_destination\features\FT-10_profile_assembly.md =====
---
feature_id: FT-10
epic: EP-04
name: "Assemble Canonical Profile"
description: |
  Construct the unified client profile including identity, identifiers,
  operational attributes, and metadata.

stories:
  - ST-20
  - ST-21

overall_status: Planned

testing_status: aggregated
halo_adherence: aggregated
guardrail_adherence: aggregated
code_quality_adherence: aggregated
security_policy_adherence: aggregated

story_statuses: {}

last_updated: <auto>
---



===== FILE: docs\mission_destination\features\FT-11_lineage_exposure.md =====
---
feature_id: FT-11
epic: EP-04
name: "Lineage Exposure"
description: |
  Display attribute-level provenance showing which source system contributed
  each value.

stories:
  - ST-22
  - ST-23

overall_status: Planned

testing_status: aggregated
halo_adherence: aggregated
guardrail_adherence: aggregated
code_quality_adherence: aggregated
security_policy_adherence: aggregated

story_statuses: {}

last_updated: <auto>
---



===== FILE: docs\mission_destination\features\FT-12_conflict_presentation.md =====
---
feature_id: FT-12
epic: EP-04
name: "Conflict Presentation"
description: |
  Highlight conflicting source attributes, display confidence scores, and show
  merge logic outcomes.

stories:
  - ST-24
  - ST-25

overall_status: Planned

testing_status: aggregated
halo_adherence: aggregated
guardrail_adherence: aggregated
code_quality_adherence: aggregated
security_policy_adherence: aggregated

story_statuses: {}

last_updated: <auto>
---



===== FILE: docs\mission_destination\features\FT-13_lineage_tracking.md =====
---
feature_id: FT-13
epic: EP-05
name: "Lineage Tracking"
description: |
  Track attribute-level lineage with timestamped provenance and update history.

stories:
  - ST-26
  - ST-27

overall_status: Planned

testing_status: aggregated
halo_adherence: aggregated
guardrail_adherence: aggregated
code_quality_adherence: aggregated
security_policy_adherence: aggregated

story_statuses: {}

last_updated: <auto>
---



===== FILE: docs\mission_destination\features\FT-14_quality_scoring.md =====
---
feature_id: FT-14
epic: EP-05
name: "Data Quality Scoring"
description: |
  Compute quality indicators including completeness, consistency, and
  freshness scores.

stories:
  - ST-28
  - ST-29

overall_status: Planned

testing_status: aggregated
halo_adherence: aggregated
guardrail_adherence: aggregated
code_quality_adherence: aggregated
security_policy_adherence: aggregated

story_statuses: {}

last_updated: <auto>
---



===== FILE: docs\mission_destination\features\FT-15_auditability.md =====
---
feature_id: FT-15
epic: EP-05
name: "Auditability & Evidence"
description: |
  Generate audit logs for ingestion, matching, merging, and profile assembly,
  and store evidence snapshots.

stories:
  - ST-30
  - ST-31

overall_status: Planned

testing_status: aggregated
halo_adherence: aggregated
guardrail_adherence: aggregated
code_quality_adherence: aggregated
security_policy_adherence: aggregated

story_statuses: {}

last_updated: <auto>
---



===== FILE: docs\mission_destination\features\FT-16_search_api.md =====
---
feature_id: FT-16
epic: EP-06
name: "Search API"
description: |
  Expose a REST endpoint for searching clients by name, identifier, or
  attributes. Includes pagination and ranked results.

stories:
  - ST-32
  - ST-33

overall_status: Planned

testing_status: aggregated
halo_adherence: aggregated
guardrail_adherence: aggregated
code_quality_adherence: aggregated
security_policy_adherence: aggregated

story_statuses: {}

last_updated: <auto>
---



===== FILE: docs\mission_destination\features\FT-17_profile_api.md =====
---
feature_id: FT-17
epic: EP-06
name: "Client Profile API"
description: |
  Expose the canonical client profile via REST, including full lineage metadata
  and data quality indicators.

stories:
  - ST-34
  - ST-35

overall_status: Planned

testing_status: aggregated
halo_adherence: aggregated
guardrail_adherence: aggregated
code_quality_adherence: aggregated
security_policy_adherence: aggregated

story_statuses: {}

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-00-backend-api-availability.md =====
---
story_id: ST-00
slug: st-00-backend-api-availability
name: Provide Basic Backend API Availability
epic: E00
feature: FT-00-backend_fundamentals
status: in_progress

# MissionSmith Status Fields (used by CI + tools/*.py)
testing_status: pass
halo_adherence: not_run
guardrail_adherence: pass
code_quality_adherence: pass
security_policy_adherence: pass
implementation_presence: partial
last_updated: 2025-12-04
---

## Description

As a developer,  
I want the backend to expose a basic availability endpoint,  
so that I can confirm the service is running and reachable.

This story establishes the foundational API contract for the backend and enables fast automated verification that the service is alive.

## Acceptance Criteria

- **AC1:** Backend can be started using the documented command (e.g. `uvicorn app_backend.main:app`).
- **AC2:** A `/health` endpoint is available.
- **AC3:** Calling `/health` returns HTTP **200**.
- **AC4:** Response JSON contains at least `{ "status": "ok" }`.
- **AC5:** A backend test exercises AC2â€“AC4.
- **AC6:** Running the story test updates `testing_status:` to `pass` when successful.

## Implementation Notes

- Touchpoint: `app_backend/main.py`
- This endpoint underpins availability checks for the entire platform and is required before any higher-level stories can be executed.







===== FILE: docs\mission_destination\stories\ST-00-frontend-ui-shell.md =====
---
story_id: ST-00-FRONTEND-UI-SHELL
slug: st-00-frontend-ui-shell
name: Provide Frontend UI Shell Availability
epic: E00
feature: FT-00-backend_fundamentals
status: in_progress

# MissionSmith Status Fields (used by CI + tools/*.py)
testing_status: pass
halo_adherence: not_run
guardrail_adherence: not_run
code_quality_adherence: pass
security_policy_adherence: pass
implementation_presence: partial
last_updated: 2025-12-04
---

## Description

As a developer,  
I want the frontend UI shell to be built and served by the backend,  
so that I can confirm the Single Client View application is accessible as a SPA.

This story verifies that the built frontend exists and the backend correctly serves
the index.html shell at the root path.

## Acceptance Criteria

- **AC1:** `npm run build` produces a frontend `dist/` folder.
- **AC2:** Backend is configured to serve the frontend build output.
- **AC3:** `GET /` returns HTTP **200**.
- **AC4:** HTML contains `<div id="root">`.
- **AC5:** HTML contains the expected page `<title>` from the built frontend.
- **AC6:** A backend test verifies AC1â€“AC5.
- **AC7:** Running the ST-00-FRONTEND-UI-SHELL tests sets `testing_status: pass`.

## Implementation Notes

- Frontend: `app_frontend/`
- Backend serving logic: `app_backend/main.py`
- Test: `tests/api/http/test_st_00_frontend_ui_shell.py`







===== FILE: docs\mission_destination\stories\ST-01_register_crm_source.md =====
---
story_id: ST-01
feature: FT-01
name: "Register CRM source"
description: |
  Register CRM system as ingestible source.

acceptance_criteria:
  - Connection succeeds
  - Schema retrieved

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-02_register_kyc_source.md =====
---
story_id: ST-02
feature: FT-01
name: "Register KYC source"
description: |
  Register KYC system.

acceptance_criteria:
  - Connection ok
  - Schema ok

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-03_map_identity_fields.md =====
---
story_id: ST-03
feature: FT-02
name: "Map identity fields"
description: |
  Map core identity fields.

acceptance_criteria:
  - Fields mapped
  - Types correct

overall_status: Planned

testing_status: pass
halo_adherence: fail
guardrail_adherence: pass
code_quality_adherence: pass
security_policy_adherence: pass
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-04_map_identifiers.md =====
---
story_id: ST-04
feature: FT-02
name: "Map identifiers"
description: |
  Map identifiers.

acceptance_criteria:
  - ID mapping valid

overall_status: Planned

testing_status: pass
halo_adherence: fail
guardrail_adherence: pass
code_quality_adherence: pass
security_policy_adherence: pass
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-05_bulk_load_crm.md =====
---
story_id: ST-05
feature: FT-03
name: "Bulk load CRM"
description: |
  Initial CRM load.

acceptance_criteria:
  - Records loaded

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-06_bulk_load_kyc.md =====
---
story_id: ST-06
feature: FT-03
name: "Bulk load KYC"
description: |
  Initial KYC load.

acceptance_criteria:
  - Records loaded

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-07_detect_upstream_deltas.md =====
---
story_id: ST-07
feature: FT-04
name: "Detect upstream deltas"
description: |
  Detect deltas.

acceptance_criteria:
  - Changes detected

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-08_apply_deltas.md =====
---
story_id: ST-08
feature: FT-04
name: "Apply deltas"
description: |
  Apply detected deltas.

acceptance_criteria:
  - Deltas applied

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-09_match_by_tax_id.md =====
---
story_id: ST-09
feature: FT-05
name: "Match by tax ID"
description: |
  Exact match by tax ID.

acceptance_criteria:
  - Matches correct

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-10_match_by_registration_number.md =====
---
story_id: ST-10
feature: FT-05
name: "Match by registration number"
description: |
  Exact match.

acceptance_criteria:
  - Matches correct

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-11_fuzzy_name_match.md =====
---
story_id: ST-11
feature: FT-06
name: "Fuzzy name match"
description: |
  Implement fuzzy name.

acceptance_criteria:
  - Similarity score valid

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-12_attribute_confidence.md =====
---
story_id: ST-12
feature: FT-06
name: "Attribute confidence"
description: |
  Compute confidence.

acceptance_criteria:
  - Confidence computed

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-13_merge_identity.md =====
---
story_id: ST-13
feature: FT-07
name: "Merge identity"
description: |
  Merge identity attributes.

acceptance_criteria:
  - Conflicts resolved

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-14_merge_addresses.md =====
---
story_id: ST-14
feature: FT-07
name: "Merge addresses"
description: |
  Merge address attributes.

acceptance_criteria:
  - Address selected

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-15_record_lineage.md =====
---
story_id: ST-15
feature: FT-07
name: "Record lineage"
description: |
  Lineage for merges.

acceptance_criteria:
  - Lineage stored

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-16_build_index.md =====
---
story_id: ST-16
feature: FT-08
name: "Build index"
description: |
  Build search index.

acceptance_criteria:
  - Index built

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-17_normalise_search_fields.md =====
---
story_id: ST-17
feature: FT-08
name: "Normalise search fields"
description: |
  Normalise fields.

acceptance_criteria:
  - Fields normalised

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-18_fuzzy_search_queries.md =====
---
story_id: ST-18
feature: FT-09
name: "Fuzzy search queries"
description: |
  Fuzzy queries.

acceptance_criteria:
  - Ranked results

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-19_search_ranking.md =====
---
story_id: ST-19
feature: FT-09
name: "Search ranking"
description: |
  Ranking logic.

acceptance_criteria:
  - Correct ordering

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-20_assemble_base_profile.md =====
---
story_id: ST-20
feature: FT-10
name: "Assemble base profile"
description: |
  Assemble profile.

acceptance_criteria:
  - Profile fields set

overall_status: Planned

testing_status: pass
halo_adherence: fail
guardrail_adherence: pass
code_quality_adherence: pass
security_policy_adherence: pass
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-21_assemble_metadata.md =====
---
story_id: ST-21
feature: FT-10
name: "Assemble metadata"
description: |
  Assemble metadata.

acceptance_criteria:
  - Metadata added

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-22_expose_lineage.md =====
---
story_id: ST-22
feature: FT-11
name: "Expose lineage"
description: |
  Lineage display.

acceptance_criteria:
  - Lineage visible

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-23_drill-down_lineage.md =====
---
story_id: ST-23
feature: FT-11
name: "Drill-down lineage"
description: |
  Drill-down.

acceptance_criteria:
  - Source-level detail

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-24_flag_conflicts.md =====
---
story_id: ST-24
feature: FT-12
name: "Flag conflicts"
description: |
  Conflict flags.

acceptance_criteria:
  - Flags correct

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-25_show_merge_logic.md =====
---
story_id: ST-25
feature: FT-12
name: "Show merge logic"
description: |
  Show logic.

acceptance_criteria:
  - Logic displayed

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-26_store_lineage_history.md =====
---
story_id: ST-26
feature: FT-13
name: "Store lineage history"
description: |
  Store history.

acceptance_criteria:
  - History stored

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-27_timestamp_lineage.md =====
---
story_id: ST-27
feature: FT-13
name: "Timestamp lineage"
description: |
  Timestamp entries.

acceptance_criteria:
  - Timestamps correct

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-28_compute_freshness.md =====
---
story_id: ST-28
feature: FT-14
name: "Compute freshness"
description: |
  Freshness score.

acceptance_criteria:
  - Score correct

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-29_compute_completeness.md =====
---
story_id: ST-29
feature: FT-14
name: "Compute completeness"
description: |
  Completeness.

acceptance_criteria:
  - Completeness correct

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-30_audit_ingestion.md =====
---
story_id: ST-30
feature: FT-15
name: "Audit ingestion"
description: |
  Audit ingestion.

acceptance_criteria:
  - Audit entries

overall_status: Planned

testing_status: pass
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: pass
security_policy_adherence: pass
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-31_audit_merge.md =====
---
story_id: ST-31
feature: FT-15
name: "Audit merge"
description: |
  Audit merge.

acceptance_criteria:
  - Audit recorded

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-32_search_api_basic.md =====
---
story_id: ST-32
feature: FT-16
name: "Search API basic"
description: |
  Search endpoint.

acceptance_criteria:
  - JSON response

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-33_search_api_ranking.md =====
---
story_id: ST-33
feature: FT-16
name: "Search API ranking"
description: |
  Ranking via API.

acceptance_criteria:
  - Ranked results

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-34_profile_api_basic.md =====
---
story_id: ST-34
feature: FT-17
name: "Profile API basic"
description: |
  Profile endpoint.

acceptance_criteria:
  - Profile returned

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_destination\stories\ST-35_profile_api_lineage.md =====
---
story_id: ST-35
feature: FT-17
name: "Profile API lineage"
description: |
  Lineage in API.

acceptance_criteria:
  - Lineage included

overall_status: Planned

testing_status: not_run
halo_adherence: fail
guardrail_adherence: fail
code_quality_adherence: fail
security_policy_adherence: fail
implementation_presence: false

last_updated: <auto>
---



===== FILE: docs\mission_framework\mission_control_status_model.md =====
# Mission Control â€” Status Tracking Model

This document defines how Mission Control tracks and updates the delivery status of **Epics**, **Features**, and **Stories** across the entire MissionSmith lifecycle. All status information is stored in **YAML front matter** within each artefact and is automatically updated through **commit processing**, **guardrail evaluation**, and **CI evidence ingestion**.

---

# 1. Status Model Overview
Each artefact (Epic, Feature, Story) maintains a consistent set of machine-evaluated statuses:

- **overall_status**
- **testing_status**
- **halo_adherence**
- **guardrail_adherence**
- **code_quality_adherence**
- **security_policy_adherence**
- **implementation_presence** (Stories only)

An artefact can only exist in one of three states:
- **Planned**
- **In Progress**
- **Complete**

Statuses are always derived from machine-readable evidence.

---

# 2. Story Status Model (Atomic Unit)
Stories are independent of one another. Story status is determined solely by its own implementation and checks.

## 2.1 Planned
- No commits to the storyâ€™s implementation slice
- No tests exist
- No adherence checks have run

## 2.2 In Progress
A Story becomes In Progress when any of the following occur:
- Commits modify the storyâ€™s implementation slice
- A test file exists but tests donâ€™t pass
- One or more adherence checks fail
- Required evidence is missing

## 2.3 Complete
A Story is Complete only when:
- Implementation presence is detected
- All tests pass
- Guardrail checks pass
- Halo checks pass
- Code quality checks pass
- Security policy checks pass

---

# 3. Feature Status Model
Feature status is aggregated from child Stories.

## 3.1 Planned
- All Stories in the Feature are Planned

## 3.2 In Progress
- At least one Story is In Progress or Complete
- Not all Stories are Complete

## 3.3 Complete
- All Stories in the Feature are Complete

---

# 4. Epic Status Model
Epic status is aggregated from child Features.

## 4.1 Planned
- All Features are Planned

## 4.2 In Progress
- At least one Feature is In Progress or Complete
- Not all Features are Complete

## 4.3 Complete
- All Features are Complete

---

# 5. Front Matter Structure

## 5.1 Story Front Matter
```yaml
story_id: ST-123
feature: FT-45

overall_status: Planned | In Progress | Complete

testing_status: pass | fail | not_run
halo_adherence: pass | fail
guardrail_adherence: pass | fail
code_quality_adherence: pass | fail
security_policy_adherence: pass | fail
implementation_presence: true | false

last_updated: 2025-01-01T12:00:00Z
```

## 5.2 Feature Front Matter
```yaml
feature_id: FT-45
epic: EP-7

overall_status: Planned | In Progress | Complete

testing_status: aggregated
halo_adherence: aggregated
guardrail_adherence: aggregated
code_quality_adherence: aggregated
security_policy_adherence: aggregated

story_statuses:
  ST-123: Complete
  ST-124: In Progress
  ST-125: Planned

last_updated: 2025-01-01T12:00:00Z
```

## 5.3 Epic Front Matter
```yaml
epic_id: EP-7

overall_status: Planned | In Progress | Complete

testing_status: aggregated
halo_adherence: aggregated
guardrail_adherence: aggregated
code_quality_adherence: aggregated
security_policy_adherence: aggregated

feature_statuses:
  FT-45: Complete
  FT-46: In Progress
  FT-47: Planned

last_updated: 2025-01-01T12:00:00Z
```

---

# 6. Status Computation Rules
1. Front matter updates occur when:
   - Story implementation slices change
   - Tests run and generate evidence
   - Guardrail checks execute
   - CI pipelines update MissionLog

2. Statuses propagate bottom-up:
   - Story â†’ Feature â†’ Epic

3. No manual modification of status fields.
4. MissionLog reflects live state directly from front matter.

---

# 7. MissionLog Reporting
MissionLog provides:
- Epic / Feature / Story roll-ups
- Per-status dashboards (testing, guardrails, halo, quality, security)
- Evidence lineage
- Completion graphs
- Compliance summaries

MissionLog stores no separate state; it visualises repo truth.

---

# 8. Principles
- Everything lives in the repo
- Status is always derived, never declared
- Stories are atomic
- Features & Epics are compositional
- Front matter is the single source of truth
- MissionLog is read-only
- Automated consistency across levels




===== FILE: docs\mission_framework\analytics\mf07_analytics.md =====

# MissionFramework Element: Analytics

## Overview
Lightweight telemetry and metrics to track behaviour, performance, and usage.

---

## Codify
- Define minimal analytics schema (events, metrics, identifiers).
- Store in mission_framework/analytics.

---

## Automate
- Code generation emits basic telemetry hooks.
- Logging patterns conform to schema.

---

## Intercept
- CI checks for missing telemetry hooks where required.
- Static analysis flags invalid schemas.

---

## Prove
- MissionAtlas (or simple dashboards) surface key metrics.
- MissionLog stores analytics evidence snapshots.




===== FILE: docs\mission_framework\application_self_healing\mf06_application_self_healing.md =====

# MissionFramework Element: Application Self-Healing

## Overview
Automated detection, response, and recovery from runtime anomalies using minimal MVP mechanisms.

---

## Codify
- Basic retry and circuit-breaker config defined in YAML.
- Minimal health-check contract established.

---

## Automate
- Generated services include retry wrappers and health checks.
- Monitoring hooks capture failures.

---

## Intercept
- Guardrails warn if required self-healing hooks missing.
- CI checks presence of retry/circuit-breaker configs.

---

## Prove
- MissionLog records health events and auto-recovery outcomes.
- Evidence snapshots show self-healing execution.




===== FILE: docs\mission_framework\business_data_lineage\mf04_business_data_lineage.md =====

# MissionFramework Element: Business Data Lineage

## Overview
Traceability of business data across sources, transformations, and outputs.

---

## Codify
- Define lineage schema (attribute source, timestamp, transformation rules).
- Stored in mission_framework/business_data_lineage.

---

## Automate
- Merge logic generates lineage metadata.
- Services automatically attach lineage info to client profiles.

---

## Intercept
- Missing lineage triggers guardrail warnings.
- CI validates completeness of lineage fields.

---

## Prove
- MissionLog lineage views show source mapping and transformation steps.
- Attribute-level lineage dashboards provide visual evidence.




===== FILE: docs\mission_framework\design_principles\mf01_design_principples.md =====

# MissionFramework Element: Design Principles

## Overview
High-level architectural principles that guide how all MissionSmith-generated applications are structured and behave.

---

## Codify
- Principles defined in YAML/MD under mission_framework/design_principles.
- Include rules for layering, statelessness, deterministic behaviour, separation of concerns.
- Machine-readable so guardrails and prompts can reference them.

---

## Automate
- Story meta-prompts enforce architectural boundaries.
- CI guardrails check for violations (e.g., direct DB calls, forbidden imports).
- Code generation scaffolds follow these principles by default.

---

## Intercept
- Guardrail engine detects structural violations.
- CI blocks merges that break design principles.
- Warnings raised for non-critical deviations.

---

## Prove
- MissionLog displays design-principle compliance per Story/Feature/Epic.
- Evidence snapshots stored for auditability.




===== FILE: docs\mission_framework\pipeline_blueprint\pipeline_blueprint_mvp.md =====

# Pipeline Blueprint â€” MissionSmith MVP CI/CD Specification

This document defines the mandatory structure, sequencing, governance, and evidence
requirements for the CI/CD pipeline used by Single Client View (SCV) and all MissionSmith-
aligned applications.

It belongs under **MissionFramework**, because it defines systemâ€‘wide governance rules for:
- how change flows through the system,
- how evidence is produced,
- how guardrails are enforced,
- how deployment occurs safely.

Recommended location:
`scv-repo/docs/mission_framework/pipeline_blueprint/pipeline_blueprint_mvp.md`

---

## 1. Pipeline Overview

The pipeline consists of six sequential stages:

1. Validate Semantic Artefacts  
2. Generate Tests  
3. Execute Tests  
4. Run Guardrails & Framework Checks  
5. Capture Evidence & Publish MissionLog Snapshot  
6. Deploy (conditional)

A failure in any stage halts all further stages.

---

## 2. Stage 1 â€” Validate Semantic Artefacts

**Purpose**  
Ensure Epics, Features, Stories, Prompts, MissionFramework artefacts, and MissionHalo
documents are structurally valid and machine-readable.

**Checks**
- YAML front matter correctness  
- story â†’ feature â†’ epic references resolve  
- required adherence fields present  
- no missing or duplicate IDs  
- correct folder structure  
- MissionFramework & MissionHalo documents present and readable  
- prompts exist and are valid  

**Evidence**
- `evidence/semantic_validation.json`  
  - pass/fail per artefact  
  - schema and structure checks  

---

## 3. Stage 2 â€” Generate Tests (from Test Meta-Prompt)

**Purpose**  
Ensure every Story has a current, valid test suite aligned to acceptance criteria.

**Behaviour**
- Call Test Meta-Prompt for:
  - Stories changed,
  - Stories missing tests,
  - Stories with implementation changes.
- Update tests only in authorised slices:
  `tests/services/<service>/test_service.py`

**Evidence**
- `evidence/test_generation.json`

---

## 4. Stage 3 â€” Execute Tests

**Purpose**  
Validate Story behaviour.

**Behaviour**
- Run pytest  
- Collect:
  - test results  
  - simple coverage metric  

**Evidence**
- `evidence/test_results.json`  
- `evidence/test_coverage.json`

---

## 5. Stage 4 â€” Run Guardrails & Framework Checks

**Purpose**  
Enforce MissionFramework systematically.

### 5.1 Technology Guardrails
- no new files/folders outside permitted slices  
- no forbidden imports  
- no cross-service coupling  
- naming conventions  
- no secrets committed  

### 5.2 Policy-as-Code
- logging format  
- PII masking  
- required audit events  

### 5.3 Business Data Lineage
- lineage metadata exists where required  
- schema alignment  

### 5.4 Technology Components Lineage
- dependency graph consistency  

### 5.5 Self-Healing (MVP)
- retry/circuit-breaker config present where appropriate  
- health-check functions present  

### 5.6 Analytics
- telemetry events present  
- analytics schema alignment  

**Evidence**
- `evidence/guardrails.json`  

---

## 6. Stage 5 â€” Capture Evidence & Publish Snapshot

**Purpose**  
Record the complete compliance and behavioural state at this commit.

**Evidence artefact**
- `evidence/snapshots/<commit_sha>.json`  
Includes:
- semantic validation  
- test results  
- guardrail outcomes  
- adherence statuses  
- version info  

This feeds MissionLog and MissionAtlas.

---

## 7. Stage 6 â€” Deploy (conditional)

Deployment only occurs if:
- stages 1â€“5 passed  
- branch is main  
- semantic versioning rules met  

**Behaviour**
- build artefact  
- publish image  
- deploy to environment  
- publish deployment evidence  

**Evidence**
- `evidence/deployment.json`  

---

## 8. Triggering Rules

- PR: run stages 1â€“4  
- Push to main: run stages 1â€“6  
- Daily schedule: run 1â€“5  
- Tag push: full pipeline including deploy  

---

## 9. Versioning Rules (MVP)

- Story implementation change â†’ patch  
- New Story â†’ minor  
- New Feature â†’ minor  
- New Epic â†’ major  
- Breaking schema/API change â†’ major  

---

## 10. Scope

This is the MVP blueprint.  
Future versions may add:
- performance tests  
- chaos engineering  
- deeper analytics  
- dynamic environment creation  
- multi-agent CI  



===== FILE: docs\mission_framework\policy_as_code\mf03_policy_as_code.md =====

# MissionFramework Element: Policy as Code

## Overview
Regulatory, compliance, and internal policy requirements expressed as executable rules to ensure automatic adherence.

---

## Codify
- Policies defined as machine-readable rules (YAML/Python checks).
- Includes PII handling, retention, security, audit trails.

---

## Automate
- Policy checks run during CI.
- Code generation injects compliance patterns (e.g., masked logs).

---

## Intercept
- Violations flagged in CI with clear messages.
- Optional â€œsoft failâ€ for non-critical policies in MVP.

---

## Prove
- MissionLog provides evidence of policy compliance.
- Reports include test results, policy execution logs, and snapshots.




===== FILE: docs\mission_framework\technology guardrails\mf02_technology_guardrails.md =====
# Technology Guardrails
## MissionFramework

### 1. Purpose
This document defines the mandatory technology constraints for all implementations within this MissionSmith application. These guardrails ensure consistency, predictability, auditability, and safe automation across all Story implementations.

---

## 2. Language and Framework Constraints
- Python is the primary implementation language.
- Only standard library + explicitly approved libraries may be used.
- No dynamic code generation at runtime.
- No reflection-based manipulation of internal structures.

---

## 3. Architectural Layering Rules

### 3.1 Domain Layer Rules (Canonical Logical Data Model)
**All services MUST operate using the canonical Logical Data Model (LDM).**
- Domain logic MUST construct, read, and write typed domain objects defined in `src/domain/models/`.
- Services MUST NOT return dictionaries, JSON, or ad-hoc structures.
- Domain objects are the single internal representation of business truth.

### 3.2 API Representation Rules
**APIs are the ONLY layer permitted to serialise domain models.**
- API slices MAY convert domain objects to dict/JSON.
- API slices MUST NOT contain business logic.
- API slices MUST NOT mutate domain semantics.

### 3.3 No Bypassing Rule
- No slice may bypass services and interact directly with raw upstream payloads.
- Domain entities MUST NOT be constructed directly in API or adapter layers.
- All transformations MUST flow through services.

---

## 4. Implementation Slice Rules
- Each Story is implemented in exactly one code slice.
- A code slice corresponds to a single Python file in the appropriate folder.
- No Story is allowed to modify more than one slice.
- No cross-slice coupling except through domain models.
- No circular imports.

---

## 5. Testing Guardrails
- Tests MUST be deterministic.
- Tests MUST be written using the test meta-prompt.
- Tests MUST verify domain behaviour, not presentation layer behaviour.
- Every Story MUST include:
  - Positive tests
  - Negative tests
  - Edge-case tests
  - Compliance with MissionFramework rules

---

## 6. CI/CD Enforcement Rules
- CI MUST run all tests for every push.
- CI MUST enforce formatting and linting.
- CI MUST fail if:
  - API implements business logic
  - Domain model is bypassed
  - A Story modifies multiple slices
  - Any guardrail is violated

---

## 7. Security and Safety Rules
- No direct file system access except via approved paths under `tools/`.
- No network calls unless explicitly authorised.
- No outbound or inbound external data inside Story implementations during MVP.

---

## 8. Change Control
- Any change to these guardrails requires:
  - Update to meta-prompts
  - Update to related tests
  - Update to dependency rules if applicable
  - Commit message referencing a Story or refactor item



===== FILE: docs\mission_framework\technology_components_lineage\mf05_technology_components_lineage.md =====

# MissionFramework Element: Technology Components Lineage

## Overview
Visibility into how system components depend on, call, or feed one another.

---

## Codify
- Dependency graph defined in YAML.
- Versioning relationships documented at component boundaries.

---

## Automate
- Schema compatibility checks run in CI.
- Dependency mismatches flagged automatically.

---

## Intercept
- CI blocks incompatible component changes.
- Runtime warnings for deprecated or unsupported paths.

---

## Prove
- MissionLog displays dependency graphs.
- Change-impact analysis available for each update.




===== FILE: docs\mission_framework\templates\front_matter\epic_front_matter_template.md =====
epic\_id: EP-XXX

name: "<Epic name>"

description: |

<High-level business or operational capability.>

<What this epic delivers and its intended outcomes.>





features:

\- FT-XXX

\- FT-YYY

\- FT-ZZZ





\# Status Fields (auto-managed)

overall\_status: Planned





testing\_status: aggregated

halo\_adherence: aggregated

guardrail\_adherence: aggregated

code\_quality\_adherence: aggregated

security\_policy\_adherence: aggregated





feature\_statuses: {}





last\_updated: <auto>




===== FILE: docs\mission_framework\templates\front_matter\feature_front_matter_template.md =====
feature\_id: FT-XXX

epic: EP-XXX

name: "<short descriptive feature name>"

description: |

<Purpose of the feature and the capability it delivers.>

<Why it exists and how it supports the epic.>





stories:

\- ST-XXX

\- ST-YYY

\- ST-ZZZ





\# Status Fields (auto-managed)

overall\_status: Planned





testing\_status: aggregated

halo\_adherence: aggregated

guardrail\_adherence: aggregated

code\_quality\_adherence: aggregated

security\_policy\_adherence: aggregated





story\_statuses: {}





last\_updated: <auto>




===== FILE: docs\mission_framework\templates\front_matter\story_front_matter_template.md =====
story\_id: ST-XXX

feature: FT-XXX

name: "<short, action-based title>"

description: |

<Human-readable description of the story.>

<What behaviour it implements and why it matters.>

acceptance\_criteria:

\- <Criterion 1>

\- <Criterion 2>

\- <Criterion 3>





\# Status Fields (auto-managed)

overall\_status: Planned





testing\_status: not\_run

halo\_adherence: fail

guardrail\_adherence: fail

code\_quality\_adherence: fail

security\_policy\_adherence: fail

implementation\_presence: false





last\_updated: <auto>




===== FILE: docs\mission_halo\mission_halo_mvp.md =====

# MissionHalo â€” MVP UX Design Principles

## Overview
MissionHalo ensures that all generated UI components share a consistent look and feel.  
For the MVP, Halo focuses exclusively on enforcing a unified visual design system:
- typography  
- colour palette  
- spacing  
- basic component styling  
- Tailwind-based enforcement  

---

## 1. Colour System

### 1.1 Core Palette
- **Primary (teal):** rgb(26,153,136) / #1A9988  
- **Secondary (orange):** rgb(235,86,0) / #EB5600  
- **Background:** white (#FFFFFF)

### 1.2 Supporting Palette
- Soft green: rgb(176,192,159) / #B0C09F  
- Soft yellow: rgb(241,205,86) / #F1CD56  
- Soft blue-grey: rgb(205,226,235) / #CDE2EB  

Usage:
- Subtle highlights  
- Card accents  
- Status tags  

### 1.3 Grey System
- Primary text: charcoal  
- Secondary text: mid-grey  
- Muted labels/help: light grey  
- Borders: very light grey  

---

## 2. Typography

### 2.1 Typefaces
- **Headings:** Raleway  
- **Body:** Lato  

### 2.2 Rules
- Max 3 font sizes per screen  
- No all-caps except tags  
- Generous line-height for clarity  

---

## 3. Layout & Spacing
- **Base spacing unit: 8px**  
- Generous whitespace  
- Group related items in white cards with soft grey borders  
- Consistent panel-level padding (16â€“24px)  

---

## 4. Components & Style

### Cards
- White or light grey background  
- Subtle border or shadow  
- Rounded corners  

### Buttons
- Primary: teal bg, white text  
- Secondary: white bg, teal border/text  
- Destructive: orange  

### Navigation
- Light backgrounds  
- Teal underline or pill for active tab  

---

## 5. Tone & Copy (MVP-level)
- Calm, precise, professional  
- Short labels, literal naming  
- Error messages state problem + next action  

---

## 6. Enforcement (Halo Adherence)
Halo adherence = PASS when:
- Components use Raleway/Lato  
- Colours come exclusively from MissionHalo palette  
- Spacing uses Tailwind's 8px-based scale  
- Components follow card/button patterns above  

Halo adherence = FAIL when:
- Arbitrary fonts are used  
- Unapproved colours appear  
- Spacing grid violated  
- Components deviate from visual rules  

---

## 7. Tailwind Configuration Snippet

```js
// tailwind.config.js
module.exports = {
  content: ["./src/**/*.{js,ts,jsx,tsx}"],
  theme: {
    extend: {
      fontFamily: {
        heading: ['Raleway', 'system-ui', 'sans-serif'],
        body: ['Lato', 'system-ui', 'sans-serif'],
      },
      colors: {
        halo: {
          primary: 'rgb(26,153,136)',     // #1A9988
          secondary: 'rgb(235,86,0)',     // #EB5600
          softGreen: 'rgb(176,192,159)',  // #B0C09F
          softYellow: 'rgb(241,205,86)',  // #F1CD56
          softBlue: 'rgb(205,226,235)',   // #CDE2EB
        },
      },
      borderRadius: {
        halo: '16px',
      },
    },
  },
  plugins: [],
};
```

---

## Storage Location
Place this file in:

```
scv-repo/docs/mission_halo/mission_halo_mvp.md
```

MissionHalo sits alongside MissionFramework as part of the system-wide governance layer.



===== FILE: docs\prompts\ci_pipeline-instruction_prompt_template.txt =====
Please validate the implementation of Story <ST-XX> using ci_pipeline_metaprompt.

Story file:
  <path to story file>

Code file:
  <path to code file>

Test file:
  <path to test file>


===== FILE: docs\prompts\ci_pipeline_metaprompt.md =====

# MissionSmith CI/CD Pipeline Meta-Prompt (MVP)

You are the MissionSmith CI/CD Pipeline Generation Engine.

Your role is to:
- READ the Pipeline Blueprint and MissionFramework documents,
- GENERATE or UPDATE the CI/CD workflow configuration (e.g. `.github/workflows/ci.yaml`),
- ENSURE the implemented pipeline matches the blueprint exactly,
- NEVER invent new rules that are not present in the blueprint or framework.

The Pipeline Blueprint and MissionFramework are the *only* sources of truth for pipeline behaviour.
This prompt must not redefine rules; it must only interpret and apply them.

---

## 1. Inputs

You will be provided with:

### 1.1 Pipeline Blueprint (MVP)
Location:
`docs/mission_framework/pipeline_blueprint/pipeline_blueprint_mvp.md`

This document defines:
- pipeline stages,
- sequencing,
- checks,
- evidence artefacts,
- triggering rules,
- versioning rules.

You must treat this as the authoritative specification of CI/CD behaviour.

### 1.2 MissionFramework Documents
Location:
`docs/mission_framework/`

Including (but not limited to):
- mf01_design_principles.md
- mf02_technology_guardrails.md
- mf03_policy_as_code.md
- mf04_business_data_lineage.md
- mf05_technology_components_lineage.md
- mf06_application_self_healing.md
- mf07_analytics.md

These define:
- guardrails,
- policy-as-code,
- lineage requirements,
- analytics,
- self-healing expectations.

### 1.3 Meta-Prompts
Location:
`docs/prompts/`

- Story code prompt: `story_code_prompt.md`
- Test code prompt: `test_code_prompt.md`

These influence:
- how code is generated,
- how tests are generated.

You must ensure the pipeline invokes these prompts where the blueprint describes their use (e.g. test generation stage).

### 1.4 Repository Structure
You may assume a standard layout including:
- `.github/workflows/` for CI configuration,
- `src/` for application code,
- `tests/` for tests,
- `evidence/` for evidence artefacts,
- `docs/` for documentation.

You must NOT change this structure unless explicitly required by the blueprint.

---

## 2. Where You May Write

You are allowed to create or modify **only** CI/CD configuration files, primarily:

- `.github/workflows/ci.yaml` (or `.yml`)

You must NOT:
- create or modify application code,
- change tests,
- alter documentation,
- introduce new non-CI files.

Your scope is strictly the CI/CD workflows needed to implement the Pipeline Blueprint.

---

## 3. How to Use the Pipeline Blueprint

You must:

1. Parse the stages defined in `pipeline_blueprint_mvp.md`.
2. For each stage:
   - map it to a job or step (or set of steps) in the CI workflow.
   - ensure ordering matches the blueprint.
   - ensure inputs/outputs match the blueprintâ€™s evidence model.
3. Implement:
   - stage 1: semantic validation,
   - stage 2: test generation (via Test Meta-Prompt or equivalent mechanism),
   - stage 3: test execution,
   - stage 4: guardrails & MissionFramework checks,
   - stage 5: evidence aggregation and snapshot,
   - stage 6: deploy (conditional).

You must not omit a required stage.  
If a stage is MVP-optional, the blueprint will say so explicitly.

---

## 4. Guardrails for CI/CD Generation

When generating or updating the CI pipeline:

- Do NOT hard-code business rules that belong in MissionFramework or the blueprint.
- Do NOT introduce extra deployment environments that are not described.
- Do NOT introduce new manual approval steps beyond what the blueprint states.
- Do NOT add unrelated jobs (e.g., linting) unless the blueprint calls for them.
- Do NOT assume a particular CI provider beyond what the repository already uses (MVP: GitHub Actions assumed).

You may:
- refactor the workflow file to improve clarity,
- split complex jobs into multiple steps,
- reuse shared actions or scripts if consistent with the blueprint.

---

## 5. Evidence & MissionLog Integration

You must ensure that the workflow:

- writes evidence artefacts to the `evidence/` directory exactly as the blueprint describes, including:
  - `semantic_validation.json`
  - `test_generation.json`
  - `test_results.json`
  - `test_coverage.json`
  - `guardrails.json`
  - `deployment.json` (for deploy runs)
  - `snapshots/<commit_sha>.json`

- ensures each job that produces evidence:
  - has clear success/failure criteria,
  - fails the pipeline on violation where required.

You do not define evidence structure here; you only ensure it is produced as per the blueprint.

---

## 6. Triggering Rules

You must implement triggers that match the blueprint:

Examples (as per MVP blueprint):
- On pull requests â†’ run non-deploy stages only.
- On push to `main` â†’ run the full pipeline including deployment.
- On schedule (e.g. daily) â†’ run validation and evidence stages without deployment.
- On tag push â†’ full pipeline including deployment.

Use the exact triggering semantics described in the blueprint.

---

## 7. Output Format

When requested to generate or update the CI/CD pipeline, you must output:

1. A short summary of what you have generated or changed.
2. The full contents of the CI workflow file.

Format:

```markdown
### Summary
<2â€“4 sentences describing the CI/CD workflow, stages, and any key details>

### Updated File: .github/workflows/ci.yaml
```yaml
# full workflow content here
```
```

No additional commentary.

---

## 8. Completion Criteria

The CI/CD configuration is considered acceptable when:

- All stages described in the Pipeline Blueprint are present.
- Their ordering is correct.
- Evidence artefacts are produced as required.
- Guardrails and policy checks are invoked where expected.
- Deployment runs only under the conditions described in the blueprint.
- Triggers (PR, push, schedule, tags) align with the blueprint.

Final enforcement is done by MissionFramework and the runtime guardrails.  
Your role is to implement the pipeline in faithful alignment with the Pipeline Blueprint.

---

## END OF META-PROMPT



===== FILE: docs\prompts\code_story_instruction_prompt_template.txt =====
Please implement Story <ST-XX> using story_code_metaprompt.

Story file:
  <path to story file>

Code file:
  <path to code file>




===== FILE: docs\prompts\story_code_metaprompt.md =====

# MissionSmith Story Implementation Meta-Prompt (MVP â€“ Guided + Flexible, Corrected)

You are the **MissionSmith Story Implementation Engine**.  
You implement one Story at a time using:

- The Story markdown file  
- The implementation slice  
- The MissionFramework documents  
- MissionHalo (UI stories only)

The Meta-Prompt is the **governing authority**.  
MissionFramework and MissionHalo define the rules.  
You must **never override or duplicate** rules from those documents.

---

## 1. Inputs

You will be given:

### 1.1 Story File  
Located under:  
`docs/mission_destination/stories/`  

It contains the authoritative definition of:
- behaviour  
- acceptance criteria  
- status/adherence fields  

You must implement the Story exactly as written.

### 1.2 Optional Feature/Epic Context  
Located under:  
`docs/mission_destination/features/`  
`docs/mission_destination/epics/`

### 1.3 Implementation Slice  
A Story must be implemented **in exactly one file**.

**Definition:**  
> The implementation slice is the specific file where the functional area of the Story already exists.  
> You may modify only this file unless the Story explicitly allows a wider scope *and* MissionFramework permits it.

If the developer provides the slice path in the instruction prompt, you must use only that file.

### 1.4 MissionFramework (governance rules)  
Located under:  
`docs/mission_framework/`

Includes design principles, guardrails, policy-as-code, lineage, self-healing, analytics.  
If this Meta-Prompt ever appears to conflict with MissionFramework, **MissionFramework wins**.

### 1.5 MissionHalo (UI only)  
Located at:  
`docs/mission_halo/mission_halo_mvp.md`

UI implementations must follow:
- Raleway (headings) / Lato (body)  
- Halo colour system  
- 8px spacing grid  
- Tailwind structure  
- Component rules  

Non-UI stories ignore MissionHalo.

---

## 2. Where You May Write Code

You may only modify:
- The single implementation slice file.

You must NOT:
- create new files  
- create new folders  
- rename files  
- modify other services  
- change schemas or APIs unless permitted by the Story *and* MissionFramework  

**Option B flexibility:**  
You may add small helper functions **within the same file** if required to satisfy the Story and allowed by MissionFramework.

---

## 3. How to Implement the Story

1. Read the Story description and acceptance criteria.
2. Consult MissionFramework documents â€” apply relevant rules.
3. If UI: consult MissionHalo â€” apply its rules.
4. Implement the simplest deterministic behaviour that satisfies the Story.
5. Stay strictly within the implementation slice.
6. Avoid inventing new behaviours or widening scope.

If the Story is ambiguous:  
**Choose the simplest compliant interpretation.**

---

## 4. What You Must Not Do

- Do not introduce requirements not in the Story.  
- Do not violate MissionFramework rules.  
- Do not drift into other files.  
- Do not create new architectural patterns.  
- Do not invent UX outside MissionHalo.  
- Do not refactor unrelated code.  

If a conflict arises:
**MissionFramework > Story > Meta-Prompt.**

---

## 5. Output Format (mandatory)

Your response MUST contain:

### 5.1 Summary (2â€“3 sentences)
Describe:
- what you implemented  
- which MissionFramework or MissionHalo considerations applied  

### 5.2 Full updated implementation file
Output the ENTIRE file.

Format:

```
### Summary
<summary>

### Updated File: src/services/<service_name>/service.py
```python
# full file content here
```
```

Do not output diffs.  
Do not output multiple files.  
Do not output commentary beyond the Summary.

---

## 6. Completion Criteria

A Story implementation is complete when:
- all acceptance criteria are met  
- code remains within the implementation slice  
- MissionFramework is respected  
- UI stories respect MissionHalo  
- code is deterministic and minimal  
- tests (generated separately) should pass  

---

## END OF META-PROMPT



===== FILE: docs\prompts\story_test_instruction_prompt_template.txt =====
Please create or update tests for Story <ST-XX> using story_test_metaprompt.

Story file:
  <path to story file>

Test file:
  <path to test file>



===== FILE: docs\prompts\story_test_metaprompt.md =====

# MissionSmith Test Implementation Meta-Prompt (MVP)

You are the MissionSmith Test Generation Engine.  
Your role is to generate deterministic, minimal, verifiable tests for a single Story using:

- the Story markdown file  
- the corresponding implementation slice  
- the MissionFramework documents  
- MissionHalo (UI stories only)

MissionFramework and MissionHalo are the governance authorities.  
This prompt must never duplicate their rulesâ€”only reference them.

---

## 1. Inputs

### 1.1 Story file
From:  
`docs/mission_destination/stories/`  

Contains behaviour and acceptance criteria.  
Tests must directly reflect these.

### 1.2 Implementation slice
Example:  
`src/services/<service_name>/service.py`

### 1.3 MissionFramework documents
From:  
`docs/mission_framework/`  

Refer to relevant rules for:
- design principles  
- guardrails  
- policy-as-code  
- lineage  
- analytics  
- self-healing patterns  

Test only *observable* effects of these rules.

### 1.4 MissionHalo (UI only)
From:  
`docs/mission_halo/mission_halo_mvp.md`

If the Story involves UI, test for adherence to Haloâ€™s:
- fonts  
- colours  
- spacing  
- Tailwind patterns  

If not UI: ignore Halo.

---

## 2. Where You May Write Tests

You may modify or create tests only inside:

```
tests/services/<service_name>/test_service.py
tests/api/<api_name>/test_<file>.py
```

You must NOT:
- create new folders  
- write cross-service tests  
- introduce new fixtures beyond existing patterns  
- test unrelated functionality  

---

## 3. How to Write Tests

### 3.1 Derive tests from acceptance criteria
Each acceptance criterion â†’ at least one deterministic test.

### 3.2 Use existing test style
Assume:
- pytest  
- no external mocking frameworks  
- follow file naming conventions  

### 3.3 Keep tests minimal
One behaviour per test unless grouping is natural.

### 3.4 Deterministic behaviour only
No randomness, timers, or external dependencies.

### 3.5 Only test what the Story declares
No speculative tests.  
No future features.  
No unrelated MissionFramework rules.  

Test MissionFramework rules *only* when they produce observable effects (e.g. lineage fields required in output).

### 3.6 UI
If the Story touches UI:
- snapshot-style structural assertions  
- Tailwind class checks  
- ensure MissionHalo visual rules appear in output  

Do not generate browser automation.

---

## 4. What You Must Not Do

- Do not test internal implementation details.  
- Do not mock unrelated services.  
- Do not test beyond the Storyâ€™s scope.  
- Do not over-test.  

---

## 5. Test Structure

Follow the Arrange â†’ Act â†’ Assert pattern:

- **Arrange:** prepare inputs.  
- **Act:** call the target function.  
- **Assert:** verify acceptance criteria.  

Assertions must be explicit and minimal.

---

## 6. Output Format

Your response must contain:

### 6.1 Summary (1â€“2 sentences)
State which acceptance criteria were converted into tests.

### 6.2 Full updated test file
Output the entire test file.

Format:

```
### Summary
<summary>

### Updated File: tests/services/<service_name>/test_service.py
```python
# full file content here
```
```

Do not output anything else.

---

## 7. Completion Criteria

A Storyâ€™s tests are complete when:
- all acceptance criteria are covered  
- tests are deterministic  
- tests reflect observable MissionFramework behaviour  
- UI tests reflect MissionHalo  
- the file is self-contained  
- the service can be validated in CI  

---

## END OF META-PROMPT



===== FILE: tests\__init__.py =====


===== FILE: tests\api\__init__.py =====




===== FILE: tests\api\http\requirements.txt =====
pytest



===== FILE: tests\api\http\test_client_profile_api.py =====
import pytest
from src.api.http.client_profile_api import ClientProfileAPI


def test_client_profile_api_uses_service_stub():
    api = ClientProfileAPI()
    with pytest.raises(NotImplementedError):
        api.get_client_profile("dummy-id")



===== FILE: tests\api\http\test_st_00_backend_api_availability.py =====
import pytest
from fastapi.testclient import TestClient

# Import the FastAPI app exactly as defined in app_backend/main.py
from app_backend.main import app


def test_health_endpoint_returns_200_and_status_ok():
    """
    ST-00: Backend API availability test.

    Verifies:
    - /health endpoint exists (AC2)
    - Returns HTTP 200 (AC3)
    - Returns JSON containing { "status": "ok" } (AC4)
    """
    client = TestClient(app)

    response = client.get("/health")

    # AC3: HTTP 200
    assert response.status_code == 200

    data = response.json()

    # AC4: JSON contains "status": "ok"
    assert isinstance(data, dict)
    assert data.get("status") == "ok"



===== FILE: tests\api\http\test_st_00_frontend_ui_shell.py =====
# tests/api/http/test_st_00_frontend_ui_shell.py
#
# Story: ST-00-frontend-ui-shell
# Purpose: Verify that the built frontend UI shell is being served by the backend.

import os

from fastapi.testclient import TestClient

from app_backend.main import app, FRONTEND_DIST  # type: ignore[import]

client = TestClient(app)


def test_frontend_shell_index_is_served():
    """
    AC2/AC3-ish smoke test:
    - Index page is served at "/"
    - HTML contains the root mount point and expected title
    """

    # If the dist folder doesn't exist, this is a clear setup failure:
    # the frontend hasn't been built.
    assert os.path.isdir(
        FRONTEND_DIST
    ), "Frontend dist missing â€“ run `npm run build` in app_frontend first."

    response = client.get("/")
    assert response.status_code == 200

    body = response.text
    # From app_frontend/index.html
    assert "<div id=\"root\">" in body
    assert "<title>SCV Frontend</title>" in body



===== FILE: tests\services\__init__.py =====


===== FILE: tests\services\audit\test_st_30_audit_ingestion.py =====
import pytest
from src.services.audit.service import AuditIngestionService

def test_record_audit_entry():
    service = AuditIngestionService()
    service.record_audit_entry(
        source="CRM",
        entity_id="client-123",
        status="success",
        details={"rows": 10}
    )
    entries = service.get_audit_entries("client-123")
    assert len(entries) == 1
    entry = entries[0]
    assert entry["source"] == "CRM"
    assert entry["entity_id"] == "client-123"
    assert entry["status"] == "success"
    assert entry["details"] == {"rows": 10}
    assert "timestamp" in entry

def test_get_audit_entries_filters_by_entity():
    service = AuditIngestionService()
    service.record_audit_entry("CRM", "client-1", "success")
    service.record_audit_entry("KYC", "client-2", "fail")
    entries = service.get_audit_entries("client-1")
    assert all(e["entity_id"] == "client-1" for e in entries)
    assert len(entries) == 1

def test_get_audit_entries_returns_all():
    service = AuditIngestionService()
    service.record_audit_entry("CRM", "client-1", "success")
    service.record_audit_entry("KYC", "client-2", "fail")
    entries = service.get_audit_entries()
    assert len(entries) == 2
    sources = {e["source"] for e in entries}
    assert sources == {"CRM", "KYC"}

def test_record_audit_entry_defaults_details():
    service = AuditIngestionService()
    service.record_audit_entry("CRM", "client-3", "success")
    entry = service.get_audit_entries("client-3")[0]
    assert entry["details"] == {}
    assert entry["status"] == "success"



===== FILE: tests\services\client_profile\test_client_profile_service.py =====
import pytest
from src.services.client_profile.service import ClientProfileService


class TestClientProfileService:
    """
    Test scaffold for ClientProfileService.

    NOTE:
    - Structure and naming follow the MissionSmith scaffold.
    - Real test cases will be generated from story definitions.
    """

    def test_get_client_profile_maps_identity_fields(self):
        service = ClientProfileService()
        # Use the mock data for client_id '123' (see service implementation)
        profile = service.get_client_profile("123")
        assert profile["name"] == "Alice Example"
        assert profile["email"] == "alice@example.com"
        assert profile["country"] in ("UK", "United Kingdom")
        assert any(i["system"] == "CRM" and i["value"] == "crm-123" for i in [id.__dict__ for id in profile["identifiers"]])
        assert any(i["system"] == "KYC" and i["value"] == "kyc-123" for i in [id.__dict__ for id in profile["identifiers"]])
        assert "CRM" in profile["raw_sources"]
        assert "KYC" in profile["raw_sources"]



===== FILE: tests\services\client_profile\test_st_04_map_identifiers.py =====
import pytest
from src.services.client_profile.service import ClientProfileService
from src.domain.models.client_profile import ClientIdentifier

def test_identifiers_mapped_from_all_sources():
    service = ClientProfileService()
    # Known client_id present in both CRM and KYC mock sources
    profile = service.get_client_profile("123")
    identifiers = profile["identifiers"]
    systems = {id_obj.system for id_obj in identifiers} if identifiers and hasattr(identifiers[0], 'system') else {id_obj['system'] for id_obj in identifiers}
    values = {id_obj.value for id_obj in identifiers} if identifiers and hasattr(identifiers[0], 'value') else {id_obj['value'] for id_obj in identifiers}
    assert systems == {"CRM", "KYC"}
    assert values == {"crm-123", "kyc-123"}
    assert len(identifiers) == 2

def test_identifiers_empty_for_unknown_client():
    service = ClientProfileService()
    profile = service.get_client_profile("notfound")
    assert profile["identifiers"] == []

def test_identifier_types():
    service = ClientProfileService()
    profile = service.get_client_profile("123")
    identifiers = profile["identifiers"]
    # Should be list of ClientIdentifier or dicts with system/value keys
    for id_obj in identifiers:
        if isinstance(id_obj, ClientIdentifier):
            assert hasattr(id_obj, "system")
            assert hasattr(id_obj, "value")
        else:
            assert "system" in id_obj
            assert "value" in id_obj



===== FILE: tests\services\client_profile\test_st_20_assemble_base_profile.py =====
import pytest
from src.services.client_profile.service import ClientProfileService

def test_assemble_base_profile_fields_set():
    service = ClientProfileService()
    # Known client_id present in both CRM and KYC mock sources
    profile = service.get_client_profile("123")
    # All base fields should be set (name, email, country)
    assert profile["name"] == "Alice Example"
    assert profile["email"] == "alice@example.com"
    assert profile["country"] == "UK"
    assert profile["client_id"] == "123"
    # Should have identifiers and addresses
    assert isinstance(profile["identifiers"], list)
    assert isinstance(profile["addresses"], list)
    # Should have lineage and raw_sources
    assert isinstance(profile["lineage"], dict)
    assert isinstance(profile["raw_sources"], dict)

def test_assemble_base_profile_unknown_client():
    service = ClientProfileService()
    profile = service.get_client_profile("notfound")
    # All base fields should be None or empty
    assert profile["name"] == ""
    assert profile["email"] is None
    assert profile["country"] is None
    assert profile["client_id"] == "notfound"
    assert profile["identifiers"] == []
    assert profile["addresses"] == []
    assert isinstance(profile["lineage"], dict)
    assert isinstance(profile["raw_sources"], dict)



===== FILE: tests\services\client_search\test_client_search_service.py =====
import pytest
from src.services.client_search.service import ClientSearchService


class TestClientSearchService:
    """
    Test scaffold for ClientSearchService.
    """

    def test_search_not_implemented(self):
        service = ClientSearchService()
        with pytest.raises(NotImplementedError):
            service.search("dummy query")




===== FILE: tools\generate_story_config_snippets.py =====
#!/usr/bin/env python
"""
Generate STORY_CONFIG snippets for a new Story.

This does NOT modify any files.
It just prints the blocks you should paste into:

- tools/run_story_tests.py
- tools/run_story_lint.py
- tools/run_story_security.py

Usage:

  python tools/generate_story_config_snippets.py \\
      ST-00 \\
      docs/mission_destination/stories/ST-00-backend-api-availability.md \\
      tests/api/http/test_st_00_backend_api_availability.py \\
      app_backend/main.py

You can pass multiple targets as comma-separated lists, e.g.:

  python tools/generate_story_config_snippets.py \\
      ST-99 \\
      docs/mission_destination/stories/ST-99_some_story.md \\
      tests/services/foo/test_st_99_foo.py,tests/api/http/test_st_99_foo_api.py \\
      src/services/foo/service.py,src/domain/models/foo.py
"""

from __future__ import annotations

import sys
from pathlib import Path


def _path_to_reproot_expr(rel_path: str) -> str:
    """
    Turn 'docs/mission_destination/stories/ST-03_map_identity_fields.md'
    into:

        REPO_ROOT
        / "docs"
        / "mission_destination"
        / "stories"
        / "ST-03_map_identity_fields.md"
    """
    parts = rel_path.replace("\\", "/").split("/")
    indent = " " * 8
    joined = ('"\n' + indent + '/ "').join(parts)
    return f"REPO_ROOT\n{indent}/ \"{joined}\""


def _format_list(name: str, items: list[str], indent_base: int = 8) -> str:
    indent = " " * indent_base
    inner = "\n".join(f'{indent}    "{item}",' for item in items)
    return f'{indent}"{name}": [\n{inner}\n{indent}],'


def main(argv: list[str]) -> int:
    if len(argv) < 5:
        print(
            "Usage:\n"
            "  python tools/generate_story_config_snippets.py "
            "STORY_ID STORY_FILE_REL_PATH PYTEST_TARGETS LINT_TARGETS [SECURITY_TARGETS]\n\n"
            "Example:\n"
            "  python tools/generate_story_config_snippets.py \\\n"
            "      ST-00 \\\n"
            "      docs/mission_destination/stories/ST-00-backend-api-availability.md \\\n"
            "      tests/api/http/test_st_00_backend_api_availability.py \\\n"
            "      app_backend/main.py\n"
        )
        return 1

    story_id = argv[1].upper()
    story_file_rel = argv[2]

    pytest_targets = [t.strip() for t in argv[3].split(",") if t.strip()]
    lint_targets = [t.strip() for t in argv[4].split(",") if t.strip()]

    if len(argv) >= 6:
        security_targets = [t.strip() for t in argv[5].split(",") if t.strip()]
    else:
        # Default: use lint targets for security if not explicitly given.
        security_targets = list(lint_targets)

    story_file_expr = _path_to_reproot_expr(story_file_rel)

    print("\n" + "=" * 72)
    print(f"STORY CONFIG SNIPPETS FOR {story_id}")
    print("=" * 72 + "\n")

    # ------------------------------------------------------------------
    # run_story_tests.py
    # ------------------------------------------------------------------
    print("# Paste into tools/run_story_tests.py inside STORY_CONFIG:")
    print()
    print(f'    "{story_id}": {{')
    print(f"        \"story_file\": {story_file_expr},")
    print(_format_list("pytest_targets", pytest_targets, indent_base=8))
    print("    },")
    print()

    # ------------------------------------------------------------------
    # run_story_lint.py
    # ------------------------------------------------------------------
    print("# Paste into tools/run_story_lint.py inside STORY_CONFIG:")
    print()
    print(f'    "{story_id}": {{')
    print(f"        \"story_file\": {story_file_expr},")
    print(_format_list("lint_targets", lint_targets, indent_base=8))
    print("    },")
    print()

    # ------------------------------------------------------------------
    # run_story_security.py
    # ------------------------------------------------------------------
    print("# Paste into tools/run_story_security.py inside STORY_CONFIG:")
    print()
    print(f'    "{story_id}": {{')
    print(f"        \"story_file\": {story_file_expr},")
    print(_format_list("security_targets", security_targets, indent_base=8))
    print("    },")
    print()

    # ------------------------------------------------------------------
    # Guardrails note
    # ------------------------------------------------------------------
    print("# NOTE: run_story_guardrails.py uses explicit _register_story(...) calls.")
    print("# If this Story should have guardrails, add a _register_story(...) block")
    print("# there manually, choosing an appropriate check function.")
    print()

    return 0


if __name__ == "__main__":
    raise SystemExit(main(sys.argv))



===== FILE: tools\repo_sync_one_liner.txt =====
git checkout main
git pull
git add .
git commit -m "Sync"
git push




===== FILE: tools\run_story_guardrails.py =====
#!/usr/bin/env python
"""
Run guardrail checks for one or more Stories and update guardrail_adherence
in each Story's front matter.

For the MVP, the guardrails we enforce are:

- For ST-00 (backend API availability):
  /health must return HTTP 200 with JSON {"status": "ok"}.

- For ST-03 / ST-04 (Client Profile stories):
  ClientProfileService.get_client_profile(...) must return a structure
  that matches the ClientProfile dataclass fields and types.
"""

from __future__ import annotations

import dataclasses
import json
import re
import sys
from pathlib import Path
from typing import Any, Dict, List, Tuple, Callable

# ---------------------------------------------------------------------------
# Repo root + sys.path fix so "src" can be imported
# ---------------------------------------------------------------------------

REPO_ROOT = Path(__file__).resolve().parents[1]

# Ensure the repo root is on sys.path so that imports work when this
# script is run as `python tools/run_story_guardrails.py`.
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

# ---------------------------------------------------------------------------
# Story configuration
# ---------------------------------------------------------------------------

# Each entry defines:
# - story_file: the markdown Story file
# - check_func: a callable that performs guardrail checks and returns (passed, message)
STORY_CONFIG: Dict[str, Dict[str, Any]] = {}


def _register_story(
    story_id: str,
    story_file: Path,
    check_func: Callable[[], Tuple[bool, str]],
) -> None:
    STORY_CONFIG[story_id] = {
        "story_file": story_file,
        "check_func": check_func,
    }


# ---------------------------------------------------------------------------
# Guardrail checks
# ---------------------------------------------------------------------------

def check_backend_health_endpoint() -> Tuple[bool, str]:
    """
    Guardrail for ST-00-backend-api-availability.

    Rules:
    - Import the FastAPI app from app_backend.main
    - Call /health via TestClient
    - Must return HTTP 200
    - JSON body must contain {"status": "ok"}
    """
    try:
        from fastapi.testclient import TestClient
        from app_backend.main import app
    except Exception as exc:  # pragma: no cover
        return False, f"Import error in health guardrail check: {exc!r}"

    try:
        client = TestClient(app)
        response = client.get("/health")
    except Exception as exc:  # pragma: no cover
        return False, f"Error calling /health in guardrail check: {exc!r}"

    if response.status_code != 200:
        return False, f"/health returned HTTP {response.status_code}, expected 200."

    try:
        data = response.json()
    except Exception as exc:  # pragma: no cover
        return False, f"/health did not return valid JSON: {exc!r}"

    if not isinstance(data, dict):
        return False, f"/health JSON payload must be an object; got {type(data).__name__}."

    status_value = data.get("status")
    if status_value != "ok":
        return False, f"/health JSON status must be 'ok'; got {status_value!r}."

    return True, "/health returns 200 with JSON {'status': 'ok'}."


def check_client_profile_data_model_adherence() -> Tuple[bool, str]:
    """
    Guardrail for ClientProfile-producing Stories (ST-03, ST-04).

    Rules:
    - get_client_profile(.) must return a dict
    - Keys of that dict must be a subset of ClientProfile fields
    - Required fields (client_id, name) must be present
    - identifiers must be a List[ClientIdentifier]
    - addresses must be a List[ClientAddress]
    - lineage, quality, metadata, raw_sources must be dicts
    """
    try:
        from src.services.client_profile.service import ClientProfileService
        from src.domain.models.client_profile import (
            ClientProfile,
            ClientIdentifier,
            ClientAddress,
        )
    except Exception as exc:  # pragma: no cover
        return False, f"Import error in guardrail check: {exc!r}"

    service = ClientProfileService()

    # Use the same client_id as the tests; this exercises the normal path
    profile = service.get_client_profile("123")

    if not isinstance(profile, dict):
        return False, "get_client_profile must return a dict derived from ClientProfile."

    # 1) Keys must match the ClientProfile dataclass (no unexpected fields)
    allowed_fields = {f.name for f in dataclasses.fields(ClientProfile)}
    keys = set(profile.keys())
    extra_fields = keys - allowed_fields
    if extra_fields:
        return False, f"Output contains fields not in ClientProfile: {sorted(extra_fields)}"

    required_fields = {"client_id", "name"}
    missing = required_fields - keys
    if missing:
        return False, f"Output is missing required fields: {sorted(missing)}"

    # 2) identifiers must be a list of ClientIdentifier instances
    identifiers = profile.get("identifiers")
    if not isinstance(identifiers, list):
        return False, "identifiers must be a list of ClientIdentifier."
    for idx, item in enumerate(identifiers):
        if not isinstance(item, ClientIdentifier):
            return False, f"identifiers[{idx}] is not a ClientIdentifier instance."

    # 3) addresses must be a list of ClientAddress instances
    addresses = profile.get("addresses")
    if not isinstance(addresses, list):
        return False, "addresses must be a list of ClientAddress."
    for idx, addr in enumerate(addresses):
        if not isinstance(addr, ClientAddress):
            return False, f"addresses[{idx}] is not a ClientAddress instance."

    # 4) lineage, quality, metadata, raw_sources should be dicts
    for field_name in ["lineage", "quality", "metadata", "raw_sources"]:
        value = profile.get(field_name)
        if not isinstance(value, dict):
            return False, f"{field_name} must be a dict; got {type(value).__name__}"

    return True, "Output adheres to ClientProfile data model."


# ---------------------------------------------------------------------------
# Register stories and their guardrails
# ---------------------------------------------------------------------------

_register_story(
    "ST-00",
    REPO_ROOT
    / "docs"
    / "mission_destination"
    / "stories"
    / "ST-00-backend-api-availability.md",
    check_backend_health_endpoint,
)

_register_story(
    "ST-03",
    REPO_ROOT
    / "docs"
    / "mission_destination"
    / "stories"
    / "ST-03_map_identity_fields.md",
    check_client_profile_data_model_adherence,
)

_register_story(
    "ST-04",
    REPO_ROOT
    / "docs"
    / "mission_destination"
    / "stories"
    / "ST-04_map_identifiers.md",
    check_client_profile_data_model_adherence,
)


# ---------------------------------------------------------------------------
# Helpers: evidence + front-matter update
# ---------------------------------------------------------------------------

def write_guardrail_evidence(
    story_id: str,
    passed: bool,
    message: str,
) -> Path:
    """
    Write a JSON evidence file for guardrail adherence for this Story.
    """
    results_dir = REPO_ROOT / "evidence" / "guardrails"
    results_dir.mkdir(parents=True, exist_ok=True)

    evidence_path = results_dir / f"{story_id}.json"
    payload = {
        "story_id": story_id,
        "passed": passed,
        "message": message,
    }
    evidence_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
    print(f">>> Wrote guardrail evidence for {story_id} to {evidence_path.relative_to(REPO_ROOT)}")
    return evidence_path


def update_story_guardrail_adherence(story_file: Path, passed: bool) -> None:
    """
    Replace the first 'guardrail_adherence: ...' line in the Story's front matter.
    """
    if not story_file.exists():
        raise FileNotFoundError(f"Story file not found: {story_file}")

    status = "pass" if passed else "fail"

    text = story_file.read_text(encoding="utf-8")

    pattern = r"(^guardrail_adherence:\s*).*$"
    replacement = rf"\1{status}"
    new_text, count = re.subn(pattern, replacement, text, count=1, flags=re.MULTILINE)

    if count == 0:
        raise RuntimeError(
            f"Story file {story_file} does not contain a guardrail_adherence line to update."
        )

    story_file.write_text(new_text, encoding="utf-8")
    rel = story_file.relative_to(REPO_ROOT)
    print(f">>> Updated {rel} -> guardrail_adherence: {status}")


def run_guardrail_for_story(story_id: str) -> Tuple[bool, str]:
    """
    Run the guardrail check for a single Story:
    - execute the check function
    - write evidence
    - update front matter
    Returns (passed, message).
    """
    config = STORY_CONFIG[story_id]
    story_file: Path = config["story_file"]  # type: ignore[assignment]
    check_func: Callable[[], Tuple[bool, str]] = config["check_func"]  # type: ignore[assignment]

    print(f">>> Running guardrail checks for Story {story_id}")
    passed, message = check_func()

    write_guardrail_evidence(story_id, passed, message)
    update_story_guardrail_adherence(story_file, passed)

    status = "pass" if passed else "fail"
    print(f">>> Story {story_id} guardrail_adherence set to {status}: {message}")
    return passed, message


# ---------------------------------------------------------------------------
# CLI entrypoint
# ---------------------------------------------------------------------------

def main(argv: List[str]) -> int:
    """
    Usage:
      python tools/run_story_guardrails.py          # run for all configured Stories
      python tools/run_story_guardrails.py ST-03    # run for a single Story
    """
    if len(argv) > 1:
        story_id = argv[1].upper()
        if story_id not in STORY_CONFIG:
            print(f"ERROR: Story {story_id!r} is not configured in STORY_CONFIG.")
            print(f"Known stories: {', '.join(sorted(STORY_CONFIG.keys()))}")
            return 1
        requested_ids = [story_id]
    else:
        requested_ids = sorted(STORY_CONFIG.keys())

    overall_exit = 0
    for sid in requested_ids:
        print(f"\n=== Guardrails for Story {sid} ===")
        passed, _ = run_guardrail_for_story(sid)
        if not passed:
            overall_exit = 1

    return overall_exit


if __name__ == "__main__":
    raise SystemExit(main(sys.argv))



===== FILE: tools\run_story_lint.py =====
#!/usr/bin/env python
"""
Run linting for one or more Stories and update code_quality_adherence
in each Story's front matter.

MVP:
- Use Ruff to lint the Story's Python files.
- If any issues are reported, mark as fail.
- Otherwise mark as pass.
"""

from __future__ import annotations

import json
import re
import subprocess
import sys
from pathlib import Path
from typing import Any, Dict, List, Tuple

# ---------------------------------------------------------------------------
# Repo root + sys.path
# ---------------------------------------------------------------------------

REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))


# ---------------------------------------------------------------------------
# Story configuration
# ---------------------------------------------------------------------------

# For each Story:
# - story_file: markdown Story file
# - lint_targets: list of Python files to lint with Ruff
STORY_CONFIG: Dict[str, Dict[str, Any]] = {
    "ST-03": {
        "story_file": REPO_ROOT
        / "docs"
        / "mission_destination"
        / "stories"
        / "ST-03_map_identity_fields.md",
        "lint_targets": [
            "src/services/client_profile/service.py",
            "src/domain/models/client_profile.py",
        ],
    },
    "ST-04": {
        "story_file": REPO_ROOT
        / "docs"
        / "mission_destination"
        / "stories"
        / "ST-04_map_identifiers.md",
        "lint_targets": [
            "src/services/client_profile/service.py",
            "src/domain/models/client_profile.py",
        ],
    },
    "ST-20": {
        "story_file": REPO_ROOT
        / "docs"
        / "mission_destination"
        / "stories"
        / "ST-20_assemble_base_profile.md",
        "lint_targets": [
            "src/services/client_profile/service.py",
            "src/domain/models/client_profile.py",
        ],
    },
    "ST-30": {
        "story_file": REPO_ROOT
        / "docs"
        / "mission_destination"
        / "stories"
        / "ST-30_audit_ingestion.md",
        "lint_targets": [
            "src/services/audit/service.py",
        ],
    },
     "ST-00": {
        "story_file": REPO_ROOT
        / "docs"
        / "mission_destination"
        / "stories"
        / "ST-00-backend-api-availability.md",
        "lint_targets": [
            "app_backend/main.py",
        ],
    },
     "ST-00-FRONTEND-UI-SHELL": {
        "story_file": REPO_ROOT
        / "docs"
        / "mission_destination"
        / "stories"
        / "ST-00-frontend-ui-shell.md",
        "lint_targets": [
            "app_backend/main.py",
        ],
    },


}


# ---------------------------------------------------------------------------
# Core helpers
# ---------------------------------------------------------------------------

def run_ruff_for_targets(targets: List[str]) -> Tuple[bool, str, List[Dict[str, Any]]]:
    """
    Run Ruff on the given Python files.

    Returns:
      (passed, message, issues)

    Where issues is a minimal list of findings (code, message, location).
    """
    cmd = [
        sys.executable,
        "-m",
        "ruff",
        "check",
        "--output-format",
        "json",
        *targets,
    ]
    print(f">>> Running Ruff: {' '.join(cmd)}")
    try:
        proc = subprocess.run(
            cmd,
            cwd=REPO_ROOT,
            capture_output=True,
            text=True,
            check=False,
        )
    except FileNotFoundError as exc:
        # Ruff not installed / importable in this environment
        return (
            False,
            f"Ruff not available: {exc!r}. Install ruff in this environment.",
            [],
        )

    stdout = proc.stdout or ""
    stderr = proc.stderr or ""

    if not stdout.strip():
        # No JSON output â€“ treat as failure with diagnostic
        return (
            False,
            f"Ruff did not produce JSON output. rc={proc.returncode}, stderr={stderr.strip()}",
            [],
        )

    try:
        data = json.loads(stdout)
    except json.JSONDecodeError as exc:
        return (
            False,
            f"Failed to parse Ruff JSON output: {exc!r}",
            [],
        )

    # Ruff JSON is a list of diagnostics
    issues: List[Dict[str, Any]] = []
    for diag in data:
        issues.append(
            {
                "filename": diag.get("filename"),
                "code": diag.get("code"),
                "message": diag.get("message"),
                "location": diag.get("location"),
            }
        )

    if issues:
        return (
            False,
            f"Ruff reported {len(issues)} code-quality issue(s).",
            issues,
        )

    return True, "No code-quality issues reported by Ruff.", issues


def write_lint_evidence(
    story_id: str,
    targets: List[str],
    passed: bool,
    message: str,
    issues: List[Dict[str, Any]],
) -> Path:
    """
    Write JSON evidence file for lint/code-quality adherence for this Story.
    """
    results_dir = REPO_ROOT / "evidence" / "lint"
    results_dir.mkdir(parents=True, exist_ok=True)

    evidence_path = results_dir / f"{story_id}.json"
    payload = {
        "story_id": story_id,
        "targets": targets,
        "passed": passed,
        "message": message,
        "issues": issues,
    }
    evidence_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
    print(f">>> Wrote lint evidence for {story_id} to {evidence_path.relative_to(REPO_ROOT)}")
    return evidence_path


def update_story_code_quality_status(story_file: Path, passed: bool) -> None:
    """
    Replace the first 'code_quality_adherence: ...' line in the Story's front matter.
    """
    if not story_file.exists():
        raise FileNotFoundError(f"Story file not found: {story_file}")

    status = "pass" if passed else "fail"

    text = story_file.read_text(encoding="utf-8")

    pattern = r"(^code_quality_adherence:\s*).*$"
    replacement = rf"\1{status}"
    new_text, count = re.subn(pattern, replacement, text, count=1, flags=re.MULTILINE)

    if count == 0:
        raise RuntimeError(
            f"Story file {story_file} does not contain a code_quality_adherence line to update."
        )

    story_file.write_text(new_text, encoding="utf-8")
    rel = story_file.relative_to(REPO_ROOT)
    print(f">>> Updated {rel} -> code_quality_adherence: {status}")


def run_lint_for_story(story_id: str) -> Tuple[bool, str]:
    """
    Run linting for a single Story:
    - execute Ruff on its targets
    - write evidence
    - update front matter

    Returns (passed, message).
    """
    config = STORY_CONFIG[story_id]
    story_file: Path = config["story_file"]  # type: ignore[assignment]
    targets: List[str] = config["lint_targets"]  # type: ignore[assignment]

    print(f">>> Running lint/code-quality checks for Story {story_id}")
    passed, message, issues = run_ruff_for_targets(targets)

    write_lint_evidence(story_id, targets, passed, message, issues)
    update_story_code_quality_status(story_file, passed)

    status = "pass" if passed else "fail"
    print(f">>> Story {story_id} code_quality_adherence set to {status}: {message}")
    return passed, message


# ---------------------------------------------------------------------------
# CLI entrypoint
# ---------------------------------------------------------------------------

def main(argv: List[str]) -> int:
    """
    Usage:
      python tools/run_story_lint.py          # run for all configured Stories
      python tools/run_story_lint.py ST-03    # run for a single Story
    """
    if len(argv) > 1:
        story_id = argv[1].upper()
        if story_id not in STORY_CONFIG:
            print(f"ERROR: Story {story_id!r} is not configured in STORY_CONFIG.")
            print(f"Known stories: {', '.join(sorted(STORY_CONFIG.keys()))}")
            return 1
        requested_ids = [story_id]
    else:
        requested_ids = sorted(STORY_CONFIG.keys())

    overall_exit = 0
    for sid in requested_ids:
        print(f"\n=== Lint/code-quality for Story {sid} ===")
        passed, _ = run_lint_for_story(sid)
        if not passed:
            overall_exit = 1

    return overall_exit


if __name__ == "__main__":
    raise SystemExit(main(sys.argv))



===== FILE: tools\run_story_security.py =====
#!/usr/bin/env python
"""
Run security checks for one or more Stories and update security_policy_adherence
in each Story's front matter.

MVP:
- Use Bandit to scan the Story's Python files for security issues.
- If any Medium/High severity issues are found, mark as fail.
- Otherwise mark as pass.
"""

from __future__ import annotations

import json
import re
import subprocess
import sys
from pathlib import Path
from typing import Any, Dict, List, Tuple

# ---------------------------------------------------------------------------
# Repo root + sys.path
# ---------------------------------------------------------------------------

REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))


# ---------------------------------------------------------------------------
# Story configuration
# ---------------------------------------------------------------------------

# For each Story:
# - story_file: markdown Story file
# - security_targets: list of Python files to scan with Bandit
STORY_CONFIG: Dict[str, Dict[str, Any]] = {
    "ST-03": {
        "story_file": REPO_ROOT
        / "docs"
        / "mission_destination"
        / "stories"
        / "ST-03_map_identity_fields.md",
        "security_targets": [
            "src/services/client_profile/service.py",
            "src/domain/models/client_profile.py",
        ],
    },
    "ST-04": {
        "story_file": REPO_ROOT
        / "docs"
        / "mission_destination"
        / "stories"
        / "ST-04_map_identifiers.md",
        "security_targets": [
            "src/services/client_profile/service.py",
            "src/domain/models/client_profile.py",
        ],
    },
    "ST-20": {
        "story_file": REPO_ROOT
        / "docs"
        / "mission_destination"
        / "stories"
        / "ST-20_assemble_base_profile.md",
        "security_targets": [
            "src/services/client_profile/service.py",
            "src/domain/models/client_profile.py",
        ],
    },
    "ST-30": {
        "story_file": REPO_ROOT
        / "docs"
        / "mission_destination"
        / "stories"
        / "ST-30_audit_ingestion.md",
        "security_targets": [
            "src/services/audit/service.py",
        ],
    },
     "ST-00": {
        "story_file": REPO_ROOT
        / "docs"
        / "mission_destination"
        / "stories"
        / "ST-00-backend-api-availability.md",
        "security_targets": [
            "app_backend/main.py",
        ],
    },
     "ST-00-FRONTEND-UI-SHELL": {
        "story_file": REPO_ROOT
        / "docs"
        / "mission_destination"
        / "stories"
        / "ST-00-frontend-ui-shell.md",
        "security_targets": [
            "app_backend/main.py",
        ],
    },


}


# ---------------------------------------------------------------------------
# Core helpers
# ---------------------------------------------------------------------------

def run_bandit_for_targets(targets: List[str]) -> Tuple[bool, str, List[Dict[str, Any]]]:
    """
    Run Bandit on the given Python files.

    Returns:
      (passed, message, issues)

    Where issues is a minimal list of findings we care about.
    """
    cmd = [
        sys.executable,
        "-m",
        "bandit",
        "-q",        # quiet banner
        "-f",
        "json",      # JSON output
        "-o",
        "-",         # write JSON to stdout
        *targets,
    ]
    print(f">>> Running Bandit: {' '.join(cmd)}")
    try:
        proc = subprocess.run(
            cmd,
            cwd=REPO_ROOT,
            capture_output=True,
            text=True,
            check=False,
        )
    except FileNotFoundError as exc:
        # bandit not installed / not importable
        return (
            False,
            f"Bandit not available: {exc!r}. Install bandit in this environment.",
            [],
        )

    stdout = proc.stdout or ""
    stderr = proc.stderr or ""

    # Bandit often returns 0 when no issues, 1 when issues found.
    # We don't rely solely on returncode; we parse JSON.
    if not stdout.strip():
        return (
            False,
            f"Bandit did not produce JSON output. rc={proc.returncode}, stderr={stderr.strip()}",
            [],
        )

    try:
        data = json.loads(stdout)
    except json.JSONDecodeError as exc:
        return (
            False,
            f"Failed to parse Bandit JSON output: {exc!r}",
            [],
        )

    raw_results = data.get("results", []) or []

    # Keep only medium/high severity & confidence findings.
    issues: List[Dict[str, Any]] = []
    for r in raw_results:
        severity = (r.get("issue_severity") or "").upper()
        confidence = (r.get("issue_confidence") or "").upper()
        if severity in {"MEDIUM", "HIGH"} and confidence in {"MEDIUM", "HIGH"}:
            issues.append(
                {
                    "filename": r.get("filename"),
                    "line_number": r.get("line_number"),
                    "test_id": r.get("test_id"),
                    "issue_severity": severity,
                    "issue_confidence": confidence,
                    "issue_text": r.get("issue_text"),
                }
            )

    if issues:
        return (
            False,
            f"Bandit found {len(issues)} medium/high security issue(s).",
            issues,
        )

    return True, "No medium/high security issues found by Bandit.", issues


def write_security_evidence(
    story_id: str,
    targets: List[str],
    passed: bool,
    message: str,
    issues: List[Dict[str, Any]],
) -> Path:
    """
    Write JSON evidence file for security adherence for this Story.
    """
    results_dir = REPO_ROOT / "evidence" / "security"
    results_dir.mkdir(parents=True, exist_ok=True)

    evidence_path = results_dir / f"{story_id}.json"
    payload = {
        "story_id": story_id,
        "targets": targets,
        "passed": passed,
        "message": message,
        "issues": issues,
    }
    evidence_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
    print(f">>> Wrote security evidence for {story_id} to {evidence_path.relative_to(REPO_ROOT)}")
    return evidence_path


def update_story_security_status(story_file: Path, passed: bool) -> None:
    """
    Replace the first 'security_policy_adherence: ...' line in the Story's front matter.
    """
    if not story_file.exists():
        raise FileNotFoundError(f"Story file not found: {story_file}")

    status = "pass" if passed else "fail"

    text = story_file.read_text(encoding="utf-8")

    pattern = r"(^security_policy_adherence:\s*).*$"
    replacement = rf"\1{status}"
    new_text, count = re.subn(pattern, replacement, text, count=1, flags=re.MULTILINE)

    if count == 0:
        raise RuntimeError(
            f"Story file {story_file} does not contain a security_policy_adherence line to update."
        )

    story_file.write_text(new_text, encoding="utf-8")
    rel = story_file.relative_to(REPO_ROOT)
    print(f">>> Updated {rel} -> security_policy_adherence: {status}")


def run_security_for_story(story_id: str) -> Tuple[bool, str]:
    """
    Run security scan for a single Story:
    - execute Bandit on its targets
    - write evidence
    - update front matter

    Returns (passed, message).
    """
    config = STORY_CONFIG[story_id]
    story_file: Path = config["story_file"]  # type: ignore[assignment]
    targets: List[str] = config["security_targets"]  # type: ignore[assignment]

    print(f">>> Running security checks for Story {story_id}")
    passed, message, issues = run_bandit_for_targets(targets)

    write_security_evidence(story_id, targets, passed, message, issues)
    update_story_security_status(story_file, passed)

    status = "pass" if passed else "fail"
    print(f">>> Story {story_id} security_policy_adherence set to {status}: {message}")
    return passed, message


# ---------------------------------------------------------------------------
# CLI entrypoint
# ---------------------------------------------------------------------------

def main(argv: List[str]) -> int:
    """
    Usage:
      python tools/run_story_security.py          # run for all configured Stories
      python tools/run_story_security.py ST-03    # run for a single Story
    """
    if len(argv) > 1:
        story_id = argv[1].upper()
        if story_id not in STORY_CONFIG:
            print(f"ERROR: Story {story_id!r} is not configured in STORY_CONFIG.")
            print(f"Known stories: {', '.join(sorted(STORY_CONFIG.keys()))}")
            return 1
        requested_ids = [story_id]
    else:
        requested_ids = sorted(STORY_CONFIG.keys())

    overall_exit = 0
    for sid in requested_ids:
        print(f"\n=== Security for Story {sid} ===")
        passed, _ = run_security_for_story(sid)
        if not passed:
            overall_exit = 1

    return overall_exit


if __name__ == "__main__":
    raise SystemExit(main(sys.argv))



===== FILE: tools\run_story_tests.py =====
#!/usr/bin/env python
"""
Run tests for one or more Stories, record results, and update testing_status
in each Story's front-matter.

For each Story:
- Run pytest on its configured targets
- Write evidence to evidence/test_results/<ST-XX>.json
- Update testing_status: pass|fail in the Story markdown front matter
"""

from __future__ import annotations

import json
import re
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Tuple

# Repo root = parent of /tools
REPO_ROOT = Path(__file__).resolve().parents[1]


# ---------------------------------------------------------------------------
# Story configuration
# ---------------------------------------------------------------------------

# As more Stories gain real tests, add them here.
STORY_CONFIG: Dict[str, Dict[str, object]] = {
    "ST-03": {
        "story_file": REPO_ROOT
        / "docs"
        / "mission_destination"
        / "stories"
        / "ST-03_map_identity_fields.md",
        # Pytest targets for this Story (should only contain tests for ST-03)
        "pytest_targets": [
            "tests/services/client_profile/test_client_profile_service.py",
        ],
    },
    "ST-04": {
        "story_file": REPO_ROOT
        / "docs"
        / "mission_destination"
        / "stories"
        / "ST-04_map_identifiers.md",
        # Separate test file focused on identifier behaviour for ST-04
        "pytest_targets": [
            "tests/services/client_profile/test_st_04_map_identifiers.py",
        ],
    },
    "ST-20": {
        "story_file": REPO_ROOT
        / "docs"
        / "mission_destination"
        / "stories"
        / "ST-20_assemble_base_profile.md",
        "pytest_targets": [
            "tests/services/client_profile/test_st_20_assemble_base_profile.py",
        ],
    },
    "ST-30": {
        "story_file": REPO_ROOT
        / "docs"
        / "mission_destination"
        / "stories"
        / "ST-30_audit_ingestion.md",
        "pytest_targets": [
            "tests/services/audit/test_st_30_audit_ingestion.py",
        ],
    },
    "ST-00": {
        "story_file": REPO_ROOT
        / "docs"
        / "mission_destination"
        / "stories"
        / "ST-00-backend-api-availability.md",
        "pytest_targets": [
            "tests/api/http/test_st_00_backend_api_availability.py",
        ],
         
    },
    "ST-00-FRONTEND-UI-SHELL": {
        "story_file": REPO_ROOT
        / "docs"
        / "mission_destination"
        / "stories"
        / "ST-00-frontend-ui-shell.md",
        "pytest_targets": [
            "tests/api/http/test_st_00_frontend_ui_shell.py",
        ],
    },






}


# ---------------------------------------------------------------------------
# Core helpers
# ---------------------------------------------------------------------------

def run_pytest_for_story(story_id: str, pytest_targets: List[str]) -> int:
    """
    Run pytest for the given Story and return its exit code.

    Uses `sys.executable -m pytest` so it works reliably on Windows, Mac, Linux.
    """
    cmd = [sys.executable, "-m", "pytest", "-q", *pytest_targets]
    print(f">>> Running tests for {story_id}: {' '.join(cmd)}")
    result = subprocess.run(cmd, cwd=REPO_ROOT, check=False)
    print(f">>> {story_id} pytest exit code: {result.returncode}")
    return result.returncode


def write_test_result_evidence(
    story_id: str,
    pytest_targets: List[str],
    exit_code: int,
    status: str,
) -> Path:
    """
    Write a small JSON evidence file with the Story's test result.
    """
    results_dir = REPO_ROOT / "evidence" / "test_results"
    results_dir.mkdir(parents=True, exist_ok=True)

    evidence_path = results_dir / f"{story_id}.json"
    payload = {
        "story_id": story_id,
        "pytest_targets": pytest_targets,
        "exit_code": exit_code,
        "status": status,
    }
    evidence_path.write_text(json.dumps(payload, indent=2), encoding="utf-8")
    print(
        ">>> Wrote test evidence for "
        f"{story_id} to {evidence_path.relative_to(REPO_ROOT)}"
    )
    return evidence_path


def update_story_testing_status(story_file: Path, status: str) -> None:
    """
    Replace the first 'testing_status: ...' line in the Story's front-matter.
    """
    if status not in {"pass", "fail"}:
        raise ValueError(f"Invalid status {status!r}; expected 'pass' or 'fail'")

    if not story_file.exists():
        raise FileNotFoundError(f"Story file not found: {story_file}")

    text = story_file.read_text(encoding="utf-8")

    pattern = r"(^testing_status:\s*).*$"
    replacement = rf"\1{status}"
    new_text, count = re.subn(pattern, replacement, text, count=1, flags=re.MULTILINE)

    if count == 0:
        raise RuntimeError(
            f"Story file {story_file} does not contain a testing_status line to update."
        )

    story_file.write_text(new_text, encoding="utf-8")
    rel = story_file.relative_to(REPO_ROOT)
    print(f">>> Updated {rel} -> testing_status: {status}")


def run_for_story(story_id: str) -> Tuple[int, str]:
    """
    Execute end-to-end for a single Story:
    - run pytest on its configured targets
    - derive status
    - write evidence
    - update front matter

    Returns (exit_code, status).
    """
    config = STORY_CONFIG[story_id]
    story_file: Path = config["story_file"]  # type: ignore[assignment]
    pytest_targets: List[str] = config["pytest_targets"]  # type: ignore[assignment]

    exit_code = run_pytest_for_story(story_id, pytest_targets)
    status = "pass" if exit_code == 0 else "fail"

    write_test_result_evidence(story_id, pytest_targets, exit_code, status)
    update_story_testing_status(story_file, status)

    print(f">>> Story {story_id} testing_status set to {status}")
    return exit_code, status


# ---------------------------------------------------------------------------
# CLI entrypoint
# ---------------------------------------------------------------------------

def main(argv: List[str]) -> int:
    """
    Usage:
      python tools/run_story_tests.py          # run for all configured Stories
      python tools/run_story_tests.py ST-03    # run for a single Story
    """
    if len(argv) > 1:
        story_id = argv[1].upper()
        if story_id not in STORY_CONFIG:
            print(f"ERROR: Story {story_id!r} is not configured in STORY_CONFIG.")
            print(f"Known stories: {', '.join(sorted(STORY_CONFIG.keys()))}")
            return 1
        requested_ids = [story_id]
    else:
        requested_ids = sorted(STORY_CONFIG.keys())

    overall_exit = 0
    for sid in requested_ids:
        print(f"\n=== Running tests for Story {sid} ===")
        exit_code, _ = run_for_story(sid)
        overall_exit = max(overall_exit, exit_code)

    return overall_exit


if __name__ == "__main__":
    raise SystemExit(main(sys.argv))



===== FILE: tools\update_testing_status.py =====
import sys
from pathlib import Path
import re

def main():
    if len(sys.argv) != 3:
        print("Usage: update_testing_status.py <story_path> <status>")
        sys.exit(1)

    story_path = Path(sys.argv[1])
    status = sys.argv[2]

    if status not in {"not_run", "pass", "fail"}:
        print(f"Invalid status: {status}")
        sys.exit(1)

    text = story_path.read_text(encoding="utf-8")

    # Replace the first occurrence of 'testing_status: ...'
    pattern = r"(^testing_status:\s*).*$"
    replacement = rf"\1{status}"
    new_text, count = re.subn(pattern, replacement, text, count=1, flags=re.MULTILINE)

    if count == 0:
        print("No testing_status line found to update.")
        sys.exit(1)

    story_path.write_text(new_text, encoding="utf-8")

if __name__ == "__main__":
    main()



